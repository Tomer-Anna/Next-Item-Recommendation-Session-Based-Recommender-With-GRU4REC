{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea4dcdd",
   "metadata": {},
   "source": [
    "# Retail Rocket\n",
    "The dataset consists of three files: a file with behaviour data (events.csv), a file with item properties (itemproperties.сsv) and a file, which describes category tree (categorytree.сsv). The data has been collected from a real-world ecommerce website. It is raw data, i.e. without any content transformations, however, all values are hashed due to confidential issues.\n",
    "\n",
    "The behaviour data, i.e. events like clicks, add to carts, transactions, represent interactions that were collected over a period of 4.5 months. A visitor can make three types of events, namely “view”, “addtocart” or “transaction”. In total there are 2 756 101 events including 2 664 312 views, 69 332 add to carts and 22 457 transactions produced by 1 407 580 unique visitors. For about 90% of events corresponding properties can be found in the “item_properties.csv” file.\n",
    "\n",
    "Took direct from [paperswithcode](https://paperswithcode.com/dataset/retailrocket) and [Kaggle](https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset)\n",
    "\n",
    "in this case im using only the interaction dataset (events.csv)\n",
    "\n",
    "## Task \n",
    "Task: Predict the next product.\n",
    "\n",
    "In this case, I'm trying to predict the next item in the session without distinguishing between the event types. Even though I split the sequences by user, and this could be a user-item problem, I solve it as an item-item problem. Given a sequence of products, I predict the next product in the session.\n",
    "\n",
    "## Solution\n",
    "To solve this task I develop a retrieval models using Two-Tower architecture:\n",
    "A query model computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
    "A candidate model computing the candidate representation (an equally-sized vector) using the candidate features\n",
    "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query.\n",
    "\n",
    "Within the framework of the Two-Tower architecture, this project specifically employs the query tower, utilizing a GRU layer to adeptly encode the sequence of historical products. The GRU's inherent ability to capture temporal dependencies proves invaluable in understanding the sequential nature of user interactions with products over time. Simultaneously, the candidate tower remains unchanged, consistently providing a robust representation of candidate products.\n",
    "\n",
    "Reference: \n",
    "- [GRU4Rec paper](https://arxiv.org/abs/1511.06939)\n",
    "- [TensorFlow Recommenders Framework](https://www.tensorflow.org/recommenders/)\n",
    "\n",
    "\n",
    "## Data Cleaning \n",
    "1. Remove items that occur consecutively in the sequence.\n",
    "2. Exclude users with minimal and excessively extensive activities.\n",
    "3. Fix columns name.\n",
    "\n",
    "## Data Preprocessing\n",
    "1. Group by user.\n",
    "2. create a sequences with length of n: [1,2,3,4,5,6,7,8] ---> [[1,2,3,4,5], [2,3,4,5,6], [3,4,5,6,7], [4,5,6,7,8]]\n",
    "3. Make the last observation (product) in each sequence as target: seq:[3,4,5,6,7] target:[8] \n",
    "4. convert the DataFrame into Tensor Dataset.\n",
    "5. Split the dataset to train validation and test (70%, 15%, 15%).\n",
    "\n",
    "## Model Training\n",
    "Training the model with: \n",
    "- window_size: 6\n",
    "- embedding size: 256 \n",
    "- batch size: 256\n",
    "- dropout: 0.25\n",
    "- learning rate: 0.06258085768442055\n",
    "\n",
    "## Evaluation\n",
    "1. Prediction example.\n",
    "2. Evaluation with relevant metrics.\n",
    "    - Recall@20 at 20: 0.3\n",
    "    - MMR@20: 0.17        \n",
    "    - NDCG@20 at 20: 0.3\n",
    "\n",
    "    \n",
    "\n",
    "## The Whole Process With Hyper Parameter Tuning\n",
    "The whole process in one piece of code including hyper parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9be88",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec253f70",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13626a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index + window_size <= len(values):\n",
    "        seq = values[start_index:start_index + window_size]\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "\n",
    "    # Padding for sequences with less than window_size\n",
    "    if start_index < len(values):\n",
    "        remaining = window_size - (len(values) - start_index)\n",
    "        padded_seq =  [0] * remaining + values[start_index:]\n",
    "        sequences.append(padded_seq)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def train_validation_test_split(map_tf_df: tf.data.Dataset,\n",
    "                                data_len: int,\n",
    "                                validation_share: float,\n",
    "                                test_share: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Split a TensorFlow dataset into train, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        map_tf_df (tf.data.Dataset): The dataset to split.\n",
    "        data_len (int): The total length of the dataset.\n",
    "        validation_share (float): The proportion of the data to allocate to the validation set (0.0 to 1.0).\n",
    "        test_share (float): The proportion of the data to allocate to the test set (0.0 to 1.0).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three datasets (train, validation, test).\n",
    "\n",
    "    \"\"\"\n",
    "    # Shuffle the dataset\n",
    "    shuffled = map_tf_df.shuffle(100_000, seed=1234, reshuffle_each_iteration=False)\n",
    "\n",
    "    # Calculate sizes for each set\n",
    "    train_size = int((1 - validation_share - test_share) * data_len)\n",
    "    validation_size = int(validation_share * data_len)\n",
    "    test_size = int(test_share * data_len)\n",
    "\n",
    "    # Create train, validation, and test sets\n",
    "    train = shuffled.take(train_size)\n",
    "    validation = shuffled.skip(train_size).take(validation_size)\n",
    "    test = shuffled.skip(train_size + validation_size).take(test_size)\n",
    "\n",
    "    # Batch and cache the datasets\n",
    "    cached_train = train.batch(4096).cache()\n",
    "    cached_validation = validation.batch(4096).cache()\n",
    "    cached_test = test.batch(4096).cache()\n",
    "\n",
    "    return cached_train, cached_validation, cached_test\n",
    "\n",
    "\n",
    "\n",
    "def accuracy_at_k_recommendations_np(predictions, true_labels, k):\n",
    "    \"\"\"\n",
    "    Calculate accuracy at k for recommendation lists using NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: NumPy array containing recommended items for each user.\n",
    "    - true_labels: NumPy array containing true items for each user.\n",
    "    - k: The value of k for accuracy calculation.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: The accuracy at k for recommendation lists.\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        # Take the top-k recommended items\n",
    "        top_k_recommendations = predictions[i, :k]\n",
    "\n",
    "        # Check if any true label is among the top-k recommendations\n",
    "        if np.any(np.isin(true_labels[i], top_k_recommendations)):\n",
    "            correct_count += 1\n",
    "\n",
    "    # Calculate accuracy at k\n",
    "    accuracy = correct_count / predictions.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def recall_at_k(predictions: np.array, targets: np.array, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Recall@k.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: 2D NumPy array, where each row represents the ranked list of relevance scores.\n",
    "                   The first element in each row is considered the most relevant.\n",
    "    - targets: 2D NumPy array, where each row represents the ground truth relevance scores.\n",
    "               It should have the same structure as the predictions.\n",
    "    - k: Integer, the cutoff rank for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - recall: Float, the Recall@k.\n",
    "    \"\"\"\n",
    "    top_k_predictions = predictions[:, :k]\n",
    "    recall = np.mean([np.isin(target, top_k_predictions[i, :]) for i, target in enumerate(targets)])\n",
    "    return recall\n",
    "\n",
    "\n",
    "def mrr_at_k(predictions: np.array, targets: np.array, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate MRR@k.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: 2D NumPy array, where each row represents the ranked list of relevance scores.\n",
    "                   The first element in each row is considered the most relevant.\n",
    "    - targets: 2D NumPy array, where each row represents the ground truth relevance scores.\n",
    "               It should have the same structure as the predictions.\n",
    "    - k: Integer, the cutoff rank for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - mrr: Float, the MRR@k.\n",
    "    \"\"\"\n",
    "    rr = []\n",
    "    for pred, target in zip(predictions[:, :k], targets):\n",
    "        if target in pred:\n",
    "            rr.append((1/(np.where(pred == target)[0][0]+1)))\n",
    "        else:\n",
    "            rr.append(0)\n",
    "    return np.mean(rr)\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_ranking, actual_ranking, k):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain at k.\n",
    "\n",
    "    Parameters:\n",
    "    - actual_ranking: List of true rankings (ground truth).\n",
    "    - predicted_ranking: List of predicted rankings.\n",
    "    - k: The position to consider in the rankings.\n",
    "\n",
    "    Returns:\n",
    "    - NDCG@k score.\n",
    "    \"\"\"\n",
    "    if len(actual_ranking) != len(predicted_ranking):\n",
    "        raise ValueError(\"Length of actual and predicted rankings must be the same.\")\n",
    "\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "\n",
    "    for i in range(len(actual_ranking)):\n",
    "        actual_rank = actual_ranking[i]\n",
    "        predicted_rank = predicted_ranking[i][:k]\n",
    "\n",
    "        # Calculate DCG (Discounted Cumulative Gain)\n",
    "        gain = 2 ** 1 - 1 if actual_rank in predicted_rank else 0\n",
    "        dcg += gain / np.log2(i + 2)\n",
    "\n",
    "        # Calculate ideal DCG for normalization\n",
    "        gain_idcg = 2 ** 1 - 1\n",
    "        idcg += gain_idcg / np.log2(i + 2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate NDCG@k\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "class GRU4REC(tfrs.Model):\n",
    "    \"\"\"\n",
    "    2 Towers model using GRU for sequential recommendations system.\n",
    "    For more details: https://www.tensorflow.org/recommenders/examples/sequential_retrieval\n",
    "\n",
    "    Attributes:\n",
    "        embedding_dimension (int): Embedding size for all the models (query and candidate).\n",
    "        items (tf.data.Dataset): MapDataset of all the items_ids.\n",
    "        add_layer (bool): Whether to add another GRU layer.\n",
    "        dropout (float): Dropout rate for GRU layers.\n",
    "        recurrent_dropout (float): Recurrent dropout rate for GRU layers.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A 2 Towers model using GRU for sequential recommendations.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dimension: int, items: tf.data.Dataset, add_layer: bool = False, dropout: float = 0.2, recurrent_dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        embedding_dimension : int\n",
    "            Embedding size for all the models (query and candidate).\n",
    "        items : tf.data.Dataset\n",
    "            MapDataset of all the items_ids.\n",
    "        add_layer : bool\n",
    "            Whether to add another GRU layer.\n",
    "        dropout : float\n",
    "            Dropout rate for GRU layers.\n",
    "        recurrent_dropout : float\n",
    "            Recurrent dropout rate for GRU layers.\n",
    "\n",
    "        Return\n",
    "        -------\n",
    "        tf.keras.Model\n",
    "        \"\"\"\n",
    "        self.items = items\n",
    "        items_ids = self.items.batch(1_000)\n",
    "        self.unique_items_ids = np.unique(np.concatenate(list(items_ids)))\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.dropout=dropout\n",
    "        self.recurrent_dropout=recurrent_dropout\n",
    "\n",
    "        if add_layer:\n",
    "            self._query_model = tf.keras.Sequential([\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=self.unique_items_ids, mask_token=None),\n",
    "                tf.keras.layers.Embedding(len(self.unique_items_ids) + 1, self.embedding_dimension),\n",
    "\n",
    "                tf.keras.layers.GRU(self.embedding_dimension, return_sequences=True, dropout=self.dropout, recurrent_dropout=self.recurrent_dropout),\n",
    "                tf.keras.layers.GRU(self.embedding_dimension, dropout=self.dropout, recurrent_dropout=self.recurrent_dropout),\n",
    "            ])\n",
    "        else:\n",
    "            self._query_model = tf.keras.Sequential([\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=self.unique_items_ids, mask_token=None),\n",
    "                tf.keras.layers.Embedding(len(self.unique_items_ids) + 1, self.embedding_dimension),\n",
    "\n",
    "                tf.keras.layers.GRU(self.embedding_dimension, dropout=self.dropout, recurrent_dropout=self.recurrent_dropout),\n",
    "            ])\n",
    "\n",
    "        self._candidate_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=self.unique_items_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(self.unique_items_ids) + 1, self.embedding_dimension)\n",
    "        ])\n",
    "        \n",
    "        self._task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "                                            ks=[1, 5, 20, 50, 100], \n",
    "                                            candidates=self.items.batch(128).map(self._candidate_model)\n",
    "                                            )\n",
    "                                         )\n",
    "\n",
    "    def compute_loss(self, features: dict, training: bool = False) -> Union[float, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the loss for the model.\n",
    "\n",
    "        Parameters:\n",
    "            features (dict): Dictionary containing input features.\n",
    "            training (bool): Flag indicating whether the model is in training mode.\n",
    "\n",
    "        Returns:\n",
    "            Union[float, tf.Tensor]: Loss value.\n",
    "        \"\"\"\n",
    "        watch_history = features[\"item_id\"]\n",
    "        watch_next_label = features[\"target\"]\n",
    "\n",
    "        query_embedding = self._query_model(watch_history)\n",
    "        candidate_embedding = self._candidate_model(watch_next_label)\n",
    "\n",
    "        return self._task(query_embedding, candidate_embedding, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0770fe6",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e2ee76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clientid</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>325215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>259884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>216305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>342816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>216305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clientid item_id\n",
       "4         2  325215\n",
       "6         2  259884\n",
       "7         2  216305\n",
       "8         2  342816\n",
       "10        2  216305"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retailrocket\n",
    "df = (pd\n",
    "        .read_csv(r'retailrocket_data\\events.csv')\n",
    "        .pipe(lambda df: df.assign(timestamp = pd.to_datetime(df['timestamp'] * 1000000)))\n",
    "        .sort_values(['visitorid', 'timestamp'])\n",
    "        .set_index(['visitorid', 'timestamp'])\n",
    "        .pipe(lambda df: df.assign(itemid_shift = df.groupby(level=0, as_index=False)['itemid'].shift()))\n",
    "        .reset_index()    \n",
    "         # drop duplicates and videos with 0 watch duration\n",
    "        .query(\"itemid_shift != itemid\")\n",
    "      )\n",
    "\n",
    "counts = df.visitorid.value_counts()\n",
    "max_limit = counts[counts<500].index.to_list()\n",
    "min_limit = counts[counts>5].index.to_list()\n",
    "\n",
    "df = (df\n",
    "        .query('visitorid in @min_limit and visitorid in @max_limit')\n",
    "        .rename(columns={'visitorid':'clientid', 'itemid':'item_id'})\n",
    "        .astype(str)\n",
    "      )[['clientid', 'item_id']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14890805",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5617e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clientid</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>[359491, 401285, 359491, 401285, 359491, 401285]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100009</td>\n",
       "      <td>[141157, 9492, 176488, 141157, 9492, 141157, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000093</td>\n",
       "      <td>[199101, 346501, 199101, 286219, 304028, 17231...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000147</td>\n",
       "      <td>[64253, 373472, 427744, 64253, 427744, 421062,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000163</td>\n",
       "      <td>[20710, 440220, 20710, 358854, 418941, 37184, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clientid                                            item_id\n",
       "0    10000   [359491, 401285, 359491, 401285, 359491, 401285]\n",
       "1   100009  [141157, 9492, 176488, 141157, 9492, 141157, 4...\n",
       "2  1000093  [199101, 346501, 199101, 286219, 304028, 17231...\n",
       "3  1000147  [64253, 373472, 427744, 64253, 427744, 421062,...\n",
       "4  1000163  [20710, 440220, 20710, 358854, 418941, 37184, ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by user\n",
    "df_group = df.groupby(\"clientid\")\n",
    "\n",
    "data = pd.DataFrame(\n",
    "                     data={\n",
    "                           \"clientid\": list(df_group.groups.keys()),\n",
    "                           \"item_id\": list(df_group.item_id.apply(list)),\n",
    "                           }\n",
    "                    )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec921b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([['359491', '401285', '359491', '401285', '359491', '401285'], [0, '401285', '359491', '401285', '359491', '401285']]),\n",
       "       list([['141157', '9492', '176488', '141157', '9492', '141157'], ['9492', '176488', '141157', '9492', '141157', '412813'], ['176488', '141157', '9492', '141157', '412813', '223829'], [0, '141157', '9492', '141157', '412813', '223829']]),\n",
       "       list([['199101', '346501', '199101', '286219', '304028', '172313'], ['346501', '199101', '286219', '304028', '172313', '346501'], ['199101', '286219', '304028', '172313', '346501', '199101'], ['286219', '304028', '172313', '346501', '199101', '192353'], ['304028', '172313', '346501', '199101', '192353', '119825'], ['172313', '346501', '199101', '192353', '119825', '346501'], ['346501', '199101', '192353', '119825', '346501', '199101'], [0, '199101', '192353', '119825', '346501', '199101']]),\n",
       "       list([['64253', '373472', '427744', '64253', '427744', '421062'], ['373472', '427744', '64253', '427744', '421062', '64253'], ['427744', '64253', '427744', '421062', '64253', '312899'], ['64253', '427744', '421062', '64253', '312899', '441901'], ['427744', '421062', '64253', '312899', '441901', '126126'], [0, '421062', '64253', '312899', '441901', '126126']]),\n",
       "       list([['20710', '440220', '20710', '358854', '418941', '37184'], ['440220', '20710', '358854', '418941', '37184', '136119'], ['20710', '358854', '418941', '37184', '136119', '223277'], [0, '358854', '418941', '37184', '136119', '223277']])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sequences with length of n for each user\n",
    "# [1,2,3,4,5,6,7,8] ---> [[1,2,3,4,5], [2,3,4,5,6], [3,4,5,6,7], [4,5,6,7,8]]\n",
    "sequence_length = 6\n",
    "step_size = 1\n",
    "data.item_id = data.item_id.apply(lambda ids: create_sequences(ids, sequence_length, step_size))\n",
    "data.head()['item_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9adbeff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clientid</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>[359491, 401285, 359491, 401285, 359491, 401285]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "      <td>[0, 401285, 359491, 401285, 359491, 401285]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100009</td>\n",
       "      <td>[141157, 9492, 176488, 141157, 9492, 141157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100009</td>\n",
       "      <td>[9492, 176488, 141157, 9492, 141157, 412813]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>[176488, 141157, 9492, 141157, 412813, 223829]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clientid                                           item_id\n",
       "0    10000  [359491, 401285, 359491, 401285, 359491, 401285]\n",
       "1    10000       [0, 401285, 359491, 401285, 359491, 401285]\n",
       "2   100009      [141157, 9492, 176488, 141157, 9492, 141157]\n",
       "3   100009      [9492, 176488, 141157, 9492, 141157, 412813]\n",
       "4   100009    [176488, 141157, 9492, 141157, 412813, 223829]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explode\n",
    "# [[1,2,3,4,5], [2,3,4,5,6], [3,4,5,6,7], [4,5,6,7,8]] --->\n",
    "#\n",
    "#0: [1,2,3,4,5]\n",
    "#0: [2,3,4,5,6]\n",
    "#0: [3,4,5,6,7]\n",
    "#0: [4,5,6,7,8]\n",
    "data_transformed = data[[\"clientid\", \"item_id\"]].explode([\"item_id\"], ignore_index=True)\n",
    "data_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7090dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[359491, 401285, 359491, 401285, 359491]</td>\n",
       "      <td>401285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 401285, 359491, 401285, 359491]</td>\n",
       "      <td>401285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[141157, 9492, 176488, 141157, 9492]</td>\n",
       "      <td>141157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[9492, 176488, 141157, 9492, 141157]</td>\n",
       "      <td>412813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[176488, 141157, 9492, 141157, 412813]</td>\n",
       "      <td>223829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    item_id  target\n",
       "0  [359491, 401285, 359491, 401285, 359491]  401285\n",
       "1       [0, 401285, 359491, 401285, 359491]  401285\n",
       "2      [141157, 9492, 176488, 141157, 9492]  141157\n",
       "3      [9492, 176488, 141157, 9492, 141157]  412813\n",
       "4    [176488, 141157, 9492, 141157, 412813]  223829"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target is the last records (item) in the list  -> item seq:[4,5,6,7,8] item target:[9]\n",
    "# convert each variable to numpy array\n",
    "data_transformed['target'] = data_transformed['item_id'].apply(lambda x : np.array(x[-1], dtype=str))\n",
    "data_transformed['item_id'] = data_transformed['item_id'].apply(lambda x : np.array(x[:-1], dtype=str))\n",
    "data_transformed = data_transformed[['item_id', 'target']]\n",
    "\n",
    "data_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138397b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec={'item_id': TensorSpec(shape=(5,), dtype=tf.string, name=None), 'target': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert each variable to tensor and store in a dict then convert to tensorflow Dataset\n",
    "data_transformed = {\n",
    "                    'item_id': tf.constant(data_transformed[\"item_id\"].to_list(), name='item_id'),\n",
    "                    'target': tf.constant(data_transformed[\"target\"], name='target')\n",
    "                   }\n",
    "\n",
    "tf_dict = tf.data.Dataset.from_tensor_slices(data_transformed)\n",
    "\n",
    "# map rows to a dictionary\n",
    "data = tf_dict.map(lambda x: {\n",
    "                               \"item_id\": x[\"item_id\"],\n",
    "                               \"target\": x[\"target\"]\n",
    "                             }\n",
    "                      )\n",
    "\n",
    "items = tf.data.Dataset.from_tensor_slices({'item_id':df.item_id.unique()}).map(lambda x: x[\"item_id\"])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9708d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id': array([b'359491', b'401285', b'359491', b'401285', b'359491'],\n",
       "       dtype=object),\n",
       " 'target': b'401285'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5892f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to train validation and test\n",
    "cached_train, cached_validation, cached_test = train_validation_test_split(data, len(data_transformed['target']), 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915062a",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d743316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TomerA\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\TomerA\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Epoch 1/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 31644.7112 - regularization_loss: 0.0000e+00 - total_loss: 31644.7112\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.12363, saving model to gru_weights\\weights01.h5\n",
      "58/58 [==============================] - 320s 5s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 31552.5957 - regularization_loss: 0.0000e+00 - total_loss: 31552.5957 - val_factorized_top_k/top_1_categorical_accuracy: 0.0116 - val_factorized_top_k/top_5_categorical_accuracy: 0.0574 - val_factorized_top_k/top_20_categorical_accuracy: 0.1236 - val_factorized_top_k/top_50_categorical_accuracy: 0.1854 - val_factorized_top_k/top_100_categorical_accuracy: 0.2375 - val_loss: 10430.8418 - val_regularization_loss: 0.0000e+00 - val_total_loss: 10430.8418 - lr: 0.0626\n",
      "Epoch 2/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 22751.2723 - regularization_loss: 0.0000e+00 - total_loss: 22751.2723\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.12363 to 0.22349, saving model to gru_weights\\weights02.h5\n",
      "58/58 [==============================] - 306s 5s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 22667.9502 - regularization_loss: 0.0000e+00 - total_loss: 22667.9502 - val_factorized_top_k/top_1_categorical_accuracy: 0.0312 - val_factorized_top_k/top_5_categorical_accuracy: 0.1256 - val_factorized_top_k/top_20_categorical_accuracy: 0.2235 - val_factorized_top_k/top_50_categorical_accuracy: 0.3002 - val_factorized_top_k/top_100_categorical_accuracy: 0.3599 - val_loss: 9241.7363 - val_regularization_loss: 0.0000e+00 - val_total_loss: 9241.7363 - lr: 0.0626\n",
      "Epoch 3/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13302.2589 - regularization_loss: 0.0000e+00 - total_loss: 13302.2589\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.22349 to 0.24322, saving model to gru_weights\\weights03.h5\n",
      "58/58 [==============================] - 338s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13236.8242 - regularization_loss: 0.0000e+00 - total_loss: 13236.8242 - val_factorized_top_k/top_1_categorical_accuracy: 0.0452 - val_factorized_top_k/top_5_categorical_accuracy: 0.1497 - val_factorized_top_k/top_20_categorical_accuracy: 0.2432 - val_factorized_top_k/top_50_categorical_accuracy: 0.3109 - val_factorized_top_k/top_100_categorical_accuracy: 0.3661 - val_loss: 9719.4238 - val_regularization_loss: 0.0000e+00 - val_total_loss: 9719.4238 - lr: 0.0626\n",
      "Epoch 4/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 6622.4812 - regularization_loss: 0.0000e+00 - total_loss: 6622.4812\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24322 to 0.28610, saving model to gru_weights\\weights04.h5\n",
      "58/58 [==============================] - 348s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 6587.7430 - regularization_loss: 0.0000e+00 - total_loss: 6587.7430 - val_factorized_top_k/top_1_categorical_accuracy: 0.0592 - val_factorized_top_k/top_5_categorical_accuracy: 0.1891 - val_factorized_top_k/top_20_categorical_accuracy: 0.2861 - val_factorized_top_k/top_50_categorical_accuracy: 0.3521 - val_factorized_top_k/top_100_categorical_accuracy: 0.4020 - val_loss: 9974.7266 - val_regularization_loss: 0.0000e+00 - val_total_loss: 9974.7266 - lr: 0.0626\n",
      "Epoch 5/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 3584.6388 - regularization_loss: 0.0000e+00 - total_loss: 3584.6388\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28610 to 0.29257, saving model to gru_weights\\weights05.h5\n",
      "58/58 [==============================] - 319s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 3568.7165 - regularization_loss: 0.0000e+00 - total_loss: 3568.7165 - val_factorized_top_k/top_1_categorical_accuracy: 0.0663 - val_factorized_top_k/top_5_categorical_accuracy: 0.2001 - val_factorized_top_k/top_20_categorical_accuracy: 0.2926 - val_factorized_top_k/top_50_categorical_accuracy: 0.3552 - val_factorized_top_k/top_100_categorical_accuracy: 0.4004 - val_loss: 10253.5020 - val_regularization_loss: 0.0000e+00 - val_total_loss: 10253.5020 - lr: 0.0626\n",
      "Epoch 6/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 2273.4231 - regularization_loss: 0.0000e+00 - total_loss: 2273.4231\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29257 to 0.29637, saving model to gru_weights\\weights06.h5\n",
      "58/58 [==============================] - 345s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 2265.0512 - regularization_loss: 0.0000e+00 - total_loss: 2265.0512 - val_factorized_top_k/top_1_categorical_accuracy: 0.0699 - val_factorized_top_k/top_5_categorical_accuracy: 0.2050 - val_factorized_top_k/top_20_categorical_accuracy: 0.2964 - val_factorized_top_k/top_50_categorical_accuracy: 0.3570 - val_factorized_top_k/top_100_categorical_accuracy: 0.4025 - val_loss: 10471.2158 - val_regularization_loss: 0.0000e+00 - val_total_loss: 10471.2158 - lr: 0.0626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 1650.0581 - regularization_loss: 0.0000e+00 - total_loss: 1650.0581\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29637 to 0.29684, saving model to gru_weights\\weights07.h5\n",
      "58/58 [==============================] - 350s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 1645.0941 - regularization_loss: 0.0000e+00 - total_loss: 1645.0941 - val_factorized_top_k/top_1_categorical_accuracy: 0.0704 - val_factorized_top_k/top_5_categorical_accuracy: 0.2086 - val_factorized_top_k/top_20_categorical_accuracy: 0.2968 - val_factorized_top_k/top_50_categorical_accuracy: 0.3565 - val_factorized_top_k/top_100_categorical_accuracy: 0.4014 - val_loss: 10643.5693 - val_regularization_loss: 0.0000e+00 - val_total_loss: 10643.5693 - lr: 0.0626\n",
      "Epoch 8/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 1326.4272 - regularization_loss: 0.0000e+00 - total_loss: 1326.4272\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.29684\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.03129043057560921.\n",
      "58/58 [==============================] - 332s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 1323.1975 - regularization_loss: 0.0000e+00 - total_loss: 1323.1975 - val_factorized_top_k/top_1_categorical_accuracy: 0.0718 - val_factorized_top_k/top_5_categorical_accuracy: 0.2104 - val_factorized_top_k/top_20_categorical_accuracy: 0.2964 - val_factorized_top_k/top_50_categorical_accuracy: 0.3560 - val_factorized_top_k/top_100_categorical_accuracy: 0.4005 - val_loss: 10769.3174 - val_regularization_loss: 0.0000e+00 - val_total_loss: 10769.3174 - lr: 0.0626\n",
      "Epoch 9/50\n",
      "58/58 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 1126.2715 - regularization_loss: 0.0000e+00 - total_loss: 1126.2715\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29684 to 0.29716, saving model to gru_weights\\weights09.h5\n",
      "58/58 [==============================] - 329s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 1123.4990 - regularization_loss: 0.0000e+00 - total_loss: 1123.4990 - val_factorized_top_k/top_1_categorical_accuracy: 0.0756 - val_factorized_top_k/top_5_categorical_accuracy: 0.2112 - val_factorized_top_k/top_20_categorical_accuracy: 0.2972 - val_factorized_top_k/top_50_categorical_accuracy: 0.3555 - val_factorized_top_k/top_100_categorical_accuracy: 0.3994 - val_loss: 10805.4932 - val_regularization_loss: 0.0000e+00 - val_total_loss: 10805.4932 - lr: 0.0313\n",
      "Epoch 9: early stopping\n",
      "CPU times: total: 2h 51min 28s\n",
      "Wall time: 49min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train The model\n",
    "model = GRU4REC(embedding_dimension=256,\n",
    "                items=items,\n",
    "                add_layer=False,\n",
    "                dropout=0.25,\n",
    "                recurrent_dropout=0.25)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.06258085768442055))\n",
    "\n",
    "# Callbacks setting\n",
    "es = EarlyStopping(monitor='val_factorized_top_k/top_20_categorical_accuracy', min_delta=0.01, patience=3, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath=r'gru_weights/weights{epoch:02d}.h5', monitor='val_factorized_top_k/top_20_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "rlr = ReduceLROnPlateau(monitor='val_factorized_top_k/top_20_categorical_accuracy', factor=0.5, patience=1, min_lr=1e-7, verbose=1)\n",
    "\n",
    "history = model.fit(cached_train, batch_size=256,\n",
    "                    epochs=50,\n",
    "                    validation_data=cached_validation, \n",
    "                    callbacks=[es, mcp, rlr],\n",
    "                    verbose=1)\n",
    "\n",
    "# Save and load the best model\n",
    "list_of_files = glob.glob('gru_weights/*')\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "model.built = True\n",
    "model.load_weights(latest_file)\n",
    "\n",
    "for f in list_of_files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47f26d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca457ced",
   "metadata": {},
   "source": [
    "Now that we have a model, we would like to be able to make predictions. \\\n",
    "We can use the `tfrs.layers.factorized_top_k.BruteForce` layer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7e2c457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x273eae5ba90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model._query_model, 50)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((items.batch(100), items.batch(100).map(model._candidate_model)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3b81c",
   "metadata": {},
   "source": [
    "### Prediction Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27566a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'167039', b'133169', b'252991', b'167039', b'133505']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(next(cached_test.as_numpy_iterator())['item_id'][42], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ff6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rec = index(np.expand_dims(next(cached_test.as_numpy_iterator())['item_id'][42], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d55ffb70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50), dtype=string, numpy=\n",
       "array([[b'164324', b'358962', b'167039', b'161780', b'147087', b'254118',\n",
       "        b'169959', b'72014', b'252991', b'441064', b'170680', b'328541',\n",
       "        b'5400', b'377140', b'153447', b'213280', b'466710', b'288174',\n",
       "        b'417464', b'113233', b'180458', b'462226', b'131868', b'213829',\n",
       "        b'401118', b'8426', b'105869', b'344592', b'187511', b'432842',\n",
       "        b'5885', b'355553', b'279271', b'309787', b'331514', b'62800',\n",
       "        b'150258', b'406601', b'92509', b'220286', b'211753', b'48747',\n",
       "        b'334476', b'165477', b'321585', b'262537', b'449897', b'97577',\n",
       "        b'435915', b'342938']], dtype=object)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624589f",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eec368af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1119s 19s/step - factorized_top_k/top_1_categorical_accuracy: 0.5208 - factorized_top_k/top_5_categorical_accuracy: 0.9894 - factorized_top_k/top_20_categorical_accuracy: 0.9986 - factorized_top_k/top_50_categorical_accuracy: 0.9996 - factorized_top_k/top_100_categorical_accuracy: 0.9998 - loss: 1051.3123 - regularization_loss: 0.0000e+00 - total_loss: 1051.3123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.520810067653656,\n",
       " 0.9893979430198669,\n",
       " 0.9985841512680054,\n",
       " 0.9995659589767456,\n",
       " 0.9998019337654114,\n",
       " 871.2779541015625,\n",
       " 0,\n",
       " 871.2779541015625]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8ce4c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 230s 18s/step - factorized_top_k/top_1_categorical_accuracy: 0.0756 - factorized_top_k/top_5_categorical_accuracy: 0.2112 - factorized_top_k/top_20_categorical_accuracy: 0.2972 - factorized_top_k/top_50_categorical_accuracy: 0.3555 - factorized_top_k/top_100_categorical_accuracy: 0.3994 - loss: 26507.2052 - regularization_loss: 0.0000e+00 - total_loss: 26507.2052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07563124597072601,\n",
       " 0.21120113134384155,\n",
       " 0.2971564531326294,\n",
       " 0.35554155707359314,\n",
       " 0.39937466382980347,\n",
       " 10805.4931640625,\n",
       " 0,\n",
       " 10805.4931640625]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef814c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 234s 18s/step - factorized_top_k/top_1_categorical_accuracy: 0.0745 - factorized_top_k/top_5_categorical_accuracy: 0.2121 - factorized_top_k/top_20_categorical_accuracy: 0.2989 - factorized_top_k/top_50_categorical_accuracy: 0.3553 - factorized_top_k/top_100_categorical_accuracy: 0.4001 - loss: 26526.3460 - regularization_loss: 0.0000e+00 - total_loss: 26526.3460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07454967498779297,\n",
       " 0.21210572123527527,\n",
       " 0.2989066243171692,\n",
       " 0.355266273021698,\n",
       " 0.4001415967941284,\n",
       " 10584.6396484375,\n",
       " 0,\n",
       " 10584.6396484375]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f25d1",
   "metadata": {},
   "source": [
    "## Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "592dff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50852/50852 [1:19:09<00:00, 10.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# make prediction, top 50, for each test observation \n",
    "test = np.array(list(cached_test.as_numpy_iterator()))\n",
    "\n",
    "item_id_values = np.concatenate([i['item_id'] for i in test]) \n",
    "target_values = np.concatenate([i['target'] for i in test]) \n",
    "\n",
    "predictions = [index(np.expand_dims(item_id_values[i], axis=0))[1].numpy()[0].tolist() for i in tqdm(range(len(target_values)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a1a2a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall_at_k\n",
      "========================================\n",
      "recall_at_k at 1: 0.12780224966569653\n",
      "recall_at_k at 5: 0.2177495477070715\n",
      "recall_at_k at 10: 0.2578462990639503\n",
      "recall_at_k at 20: 0.300479823802407\n",
      "recall_at_k at 50: 0.3558758750884921\n",
      "\n",
      "mrr_at_k\n",
      "========================================\n",
      "mrr_at_k at 1: 0.12780224966569653\n",
      "mrr_at_k at 5: 0.16089534334932748\n",
      "mrr_at_k at 10: 0.16629709902000075\n",
      "mrr_at_k at 20: 0.1692401719372935\n",
      "mrr_at_k at 50: 0.17101372775031431\n",
      "\n",
      "ndcg_at_k\n",
      "========================================\n",
      "ndcg_at_k at 1: 0.1273329215739792\n",
      "ndcg_at_k at 5: 0.2174988866006147\n",
      "ndcg_at_k at 10: 0.2572979629559524\n",
      "ndcg_at_k at 20: 0.2998971152673133\n",
      "ndcg_at_k at 50: 0.35522194169072074\n"
     ]
    }
   ],
   "source": [
    "for func in [recall_at_k, mrr_at_k, ndcg_at_k]:\n",
    "    print(f\"\\n{func.__name__}\")\n",
    "    print(\"=\"*40)\n",
    "    for k in [1, 5, 10, 20, 50]:\n",
    "        print(f\"{func.__name__} at {k}: {func(np.array(predictions), target_values, k=k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6adec1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for func in [recall_at_k, mrr_at_k, ndcg_at_k]:\n",
    "    y=[func(np.array(predictions), target_values, k=k) for k in range(1, 51)]\n",
    "    x=range(1, 51)\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f'{func.__name__}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbec460",
   "metadata": {},
   "source": [
    "## The Whole Process With Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cc3894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 16:49:22,757] A new study created in memory with name: no-name-c2284ca7-8f55-41d1-81a2-ce465c91887a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03765, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03765 to 0.05184, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05184 to 0.06578, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06578 to 0.07573, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07573 to 0.08122, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08122 to 0.08665, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08665 to 0.08963, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08963 to 0.09229, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09229 to 0.09381, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09381 to 0.09599, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09599 to 0.09701, saving model to gru_weights\\weights11.h5\n",
      "Epoch 11: early stopping\n",
      "gru_weights\\weights11.h5\n",
      "11/11 [==============================] - 85s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.0973 - loss: 29264.8188 - regularization_loss: 0.0000e+00 - total_loss: 29264.8188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 17:16:26,365] Trial 0 finished with value: 0.09729789942502975 and parameters: {'window_size': 7, 'embedding_dimension': 128, 'dropout': 0.2, 'learning_rate': 0.016124591095265373, 'batch_size': 32}. Best is trial 0 with value: 0.09729789942502975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00030, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00030 to 0.00033, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00014937091327738017.\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00033 to 0.00040, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 7.468545663869008e-05.\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00040 to 0.00043, saving model to gru_weights\\weights04.h5\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights04.h5\n",
      "8/8 [==============================] - 66s 8s/step - factorized_top_k/top_20_categorical_accuracy: 3.6615e-04 - loss: 28697.0955 - regularization_loss: 0.0000e+00 - total_loss: 28697.0955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 17:26:17,926] Trial 1 finished with value: 0.0003661540395114571 and parameters: {'window_size': 12, 'embedding_dimension': 128, 'dropout': 0.05, 'learning_rate': 0.00029874182104867164, 'batch_size': 32}. Best is trial 0 with value: 0.09729789942502975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00139, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00139 to 0.00468, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00468 to 0.00670, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00670 to 0.00990, saving model to gru_weights\\weights04.h5\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights04.h5\n",
      "6/6 [==============================] - 115s 19s/step - factorized_top_k/top_20_categorical_accuracy: 0.0107 - loss: 39709.9040 - regularization_loss: 0.0000e+00 - total_loss: 39709.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 17:56:44,622] Trial 2 finished with value: 0.010662059299647808 and parameters: {'window_size': 17, 'embedding_dimension': 512, 'dropout': 0.30000000000000004, 'learning_rate': 0.04337743182470487, 'batch_size': 256}. Best is trial 0 with value: 0.09729789942502975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.04807, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04807 to 0.11795, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11795 to 0.17453, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17453 to 0.20751, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20751 to 0.22465, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.22465 to 0.23415, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23415 to 0.23957, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23957 to 0.24252, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24252 to 0.24675, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24675 to 0.24765, saving model to gru_weights\\weights10.h5\n",
      "Epoch 10: early stopping\n",
      "gru_weights\\weights10.h5\n",
      "10/10 [==============================] - 114s 11s/step - factorized_top_k/top_20_categorical_accuracy: 0.2446 - loss: 24396.7954 - regularization_loss: 0.0000e+00 - total_loss: 24396.7954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 18:22:35,289] Trial 3 finished with value: 0.2445949912071228 and parameters: {'window_size': 9, 'embedding_dimension': 128, 'dropout': 0.45, 'learning_rate': 0.04451347975435398, 'batch_size': 64}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00257, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00257 to 0.01001, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01001 to 0.01506, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01506 to 0.01588, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01588 to 0.01664, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01664\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0011173379607498646.\n",
      "Epoch 6: early stopping\n",
      "gru_weights\\weights05.h5\n",
      "9/9 [==============================] - 88s 10s/step - factorized_top_k/top_20_categorical_accuracy: 0.0178 - loss: 29111.5576 - regularization_loss: 0.0000e+00 - total_loss: 29111.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 18:37:22,255] Trial 4 finished with value: 0.01777583174407482 and parameters: {'window_size': 10, 'embedding_dimension': 128, 'dropout': 0.55, 'learning_rate': 0.0022346758408902253, 'batch_size': 256}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.01686, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01686 to 0.03269, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03269 to 0.03820, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03820 to 0.04289, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04289 to 0.04792, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04792 to 0.05130, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05130 to 0.05540, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05540 to 0.05941, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05941 to 0.06159, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06159 to 0.06479, saving model to gru_weights\\weights10.h5\n",
      "Epoch 10: early stopping\n",
      "gru_weights\\weights10.h5\n",
      "8/8 [==============================] - 59s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.0663 - loss: 29281.7500 - regularization_loss: 0.0000e+00 - total_loss: 29281.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 18:54:04,095] Trial 5 finished with value: 0.06631839275360107 and parameters: {'window_size': 11, 'embedding_dimension': 64, 'dropout': 0.55, 'learning_rate': 0.021087639556375753, 'batch_size': 1024}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00014, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00014 to 0.00102, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00102 to 0.00211, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00211 to 0.00507, saving model to gru_weights\\weights04.h5\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights04.h5\n",
      "7/7 [==============================] - 50s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.0054 - loss: 33453.3237 - regularization_loss: 0.0000e+00 - total_loss: 33453.3237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 18:59:38,454] Trial 6 finished with value: 0.00538599630817771 and parameters: {'window_size': 13, 'embedding_dimension': 32, 'dropout': 0.55, 'learning_rate': 0.001702328279090581, 'batch_size': 32}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00942, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00942 to 0.01767, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01767 to 0.01889, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01889\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0033502918668091297.\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights03.h5\n",
      "8/8 [==============================] - 52s 6s/step - factorized_top_k/top_20_categorical_accuracy: 0.0191 - loss: 32031.0877 - regularization_loss: 0.0000e+00 - total_loss: 32031.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 19:05:35,622] Trial 7 finished with value: 0.019050896167755127 and parameters: {'window_size': 11, 'embedding_dimension': 32, 'dropout': 0.65, 'learning_rate': 0.006700583949302325, 'batch_size': 64}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00032, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.00032\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 1.7181253497255966e-05.\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00032 to 0.00033, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 8.590626748627983e-06.\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.00033\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.2953133743139915e-06.\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights03.h5\n",
      "14/14 [==============================] - 117s 8s/step - factorized_top_k/top_20_categorical_accuracy: 2.6436e-04 - loss: 33325.2586 - regularization_loss: 0.0000e+00 - total_loss: 33325.2586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 19:21:12,219] Trial 8 finished with value: 0.0002643637708388269 and parameters: {'window_size': 5, 'embedding_dimension': 256, 'dropout': 0.1, 'learning_rate': 3.436250562908517e-05, 'batch_size': 32}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00790, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00790 to 0.01590, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01590\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.002344863722100854.\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01590\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.001172431861050427.\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights02.h5\n",
      "14/14 [==============================] - 96s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.0173 - loss: 33285.3828 - regularization_loss: 0.0000e+00 - total_loss: 33285.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 19:31:02,540] Trial 9 finished with value: 0.017307015135884285 and parameters: {'window_size': 5, 'embedding_dimension': 64, 'dropout': 0.7, 'learning_rate': 0.00468972767413828, 'batch_size': 1024}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.01760, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01760 to 0.03308, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03308 to 0.08934, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08934 to 0.11391, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11391 to 0.13595, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13595 to 0.14319, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14319 to 0.14722, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14722 to 0.14813, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14813 to 0.14976, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14976 to 0.15043, saving model to gru_weights\\weights10.h5\n",
      "Epoch 10: early stopping\n",
      "gru_weights\\weights10.h5\n",
      "6/6 [==============================] - 66s 11s/step - factorized_top_k/top_20_categorical_accuracy: 0.1474 - loss: 32906.0324 - regularization_loss: 0.0000e+00 - total_loss: 32906.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 20:25:01,612] Trial 10 finished with value: 0.14740224182605743 and parameters: {'window_size': 19, 'embedding_dimension': 512, 'dropout': 0.4, 'learning_rate': 0.08624264297617684, 'batch_size': 128}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00365, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00365 to 0.00827, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00827 to 0.00967, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00967 to 0.01168, saving model to gru_weights\\weights04.h5\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights04.h5\n",
      "6/6 [==============================] - 66s 11s/step - factorized_top_k/top_20_categorical_accuracy: 0.0115 - loss: 83189.3259 - regularization_loss: 0.0000e+00 - total_loss: 83189.3259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 20:47:41,331] Trial 11 finished with value: 0.011544213630259037 and parameters: {'window_size': 20, 'embedding_dimension': 512, 'dropout': 0.4, 'learning_rate': 0.07780885673807027, 'batch_size': 128}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00215, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00215 to 0.01065, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01065 to 0.01725, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01725 to 0.02949, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.02949 to 0.04184, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04184 to 0.05946, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05946 to 0.06784, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06784 to 0.07449, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07449 to 0.07886, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07886 to 0.08141, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08141 to 0.08477, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08477 to 0.08668, saving model to gru_weights\\weights12.h5\n",
      "\n",
      "Epoch 13: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08668 to 0.08797, saving model to gru_weights\\weights13.h5\n",
      "\n",
      "Epoch 14: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08797 to 0.08887, saving model to gru_weights\\weights14.h5\n",
      "Epoch 14: early stopping\n",
      "gru_weights\\weights14.h5\n",
      "7/7 [==============================] - 79s 11s/step - factorized_top_k/top_20_categorical_accuracy: 0.0868 - loss: 32121.8480 - regularization_loss: 0.0000e+00 - total_loss: 32121.8480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 22:09:13,182] Trial 12 finished with value: 0.08679979294538498 and parameters: {'window_size': 16, 'embedding_dimension': 512, 'dropout': 0.4, 'learning_rate': 0.07186013665886604, 'batch_size': 64}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03718, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03718 to 0.06791, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06791 to 0.09939, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09939 to 0.14091, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14091 to 0.16476, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16476 to 0.18033, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18033 to 0.18641, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18641 to 0.18974, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18974 to 0.19011, saving model to gru_weights\\weights09.h5\n",
      "Epoch 9: early stopping\n",
      "gru_weights\\weights09.h5\n",
      "10/10 [==============================] - 87s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.1946 - loss: 33877.3129 - regularization_loss: 0.0000e+00 - total_loss: 33877.3129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 22:35:36,345] Trial 13 finished with value: 0.19462822377681732 and parameters: {'window_size': 8, 'embedding_dimension': 256, 'dropout': 0.30000000000000004, 'learning_rate': 0.09572562802609598, 'batch_size': 128}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.02809, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.02809 to 0.06199, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06199 to 0.09099, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09099 to 0.11462, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11462 to 0.13212, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13212 to 0.14516, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14516 to 0.15281, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15281 to 0.16088, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16088 to 0.16575, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16575 to 0.16799, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16799 to 0.17273, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17273 to 0.17478, saving model to gru_weights\\weights12.h5\n",
      "\n",
      "Epoch 13: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17478 to 0.17900, saving model to gru_weights\\weights13.h5\n",
      "\n",
      "Epoch 14: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17900 to 0.18135, saving model to gru_weights\\weights14.h5\n",
      "Epoch 14: early stopping\n",
      "gru_weights\\weights14.h5\n",
      "10/10 [==============================] - 78s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.1855 - loss: 30393.2131 - regularization_loss: 0.0000e+00 - total_loss: 30393.2131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 23:14:02,955] Trial 14 finished with value: 0.18549422919750214 and parameters: {'window_size': 8, 'embedding_dimension': 256, 'dropout': 0.25, 'learning_rate': 0.018441981947679622, 'batch_size': 4096}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.01170, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01170 to 0.03134, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03134 to 0.04270, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04270 to 0.05075, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05075 to 0.05753, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05753 to 0.06140, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06140 to 0.06598, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06598 to 0.06925, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06925 to 0.07297, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07297 to 0.07386, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07386 to 0.07559, saving model to gru_weights\\weights11.h5\n",
      "Epoch 11: early stopping\n",
      "gru_weights\\weights11.h5\n",
      "11/11 [==============================] - 86s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.0737 - loss: 30869.1175 - regularization_loss: 0.0000e+00 - total_loss: 30869.1175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-06 23:44:23,614] Trial 15 finished with value: 0.07365728914737701 and parameters: {'window_size': 7, 'embedding_dimension': 256, 'dropout': 0.15000000000000002, 'learning_rate': 0.00818227119278255, 'batch_size': 16384}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03960, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03960 to 0.06961, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06961 to 0.09577, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09577 to 0.11941, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11941 to 0.13528, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13528 to 0.14457, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14457 to 0.15326, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15326 to 0.15895, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15895 to 0.16389, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16389 to 0.16681, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16681 to 0.17053, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17053 to 0.17183, saving model to gru_weights\\weights12.h5\n",
      "Epoch 12: early stopping\n",
      "gru_weights\\weights12.h5\n",
      "10/10 [==============================] - 62s 6s/step - factorized_top_k/top_20_categorical_accuracy: 0.1706 - loss: 24763.2771 - regularization_loss: 0.0000e+00 - total_loss: 24763.2771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 00:05:53,691] Trial 16 finished with value: 0.1705579161643982 and parameters: {'window_size': 9, 'embedding_dimension': 128, 'dropout': 0.45, 'learning_rate': 0.028615614142548842, 'batch_size': 256}. Best is trial 3 with value: 0.2445949912071228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.17117, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17117 to 0.23917, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23917 to 0.25089, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25089 to 0.26763, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26763 to 0.27167, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27167 to 0.27306, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27306 to 0.27424, saving model to gru_weights\\weights07.h5\n",
      "Epoch 7: early stopping\n",
      "gru_weights\\weights07.h5\n",
      "16/16 [==============================] - 119s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.2700 - loss: 27556.6849 - regularization_loss: 0.0000e+00 - total_loss: 27556.6849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 00:29:42,984] Trial 17 finished with value: 0.2700389623641968 and parameters: {'window_size': 4, 'embedding_dimension': 256, 'dropout': 0.30000000000000004, 'learning_rate': 0.08748744601208104, 'batch_size': 256}. Best is trial 17 with value: 0.2700389623641968.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.00029, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00029 to 0.00032, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0002300897758686915.\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00032 to 0.00034, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00011504488793434575.\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.00034 to 0.00035, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 5.7522443967172876e-05.\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights04.h5\n",
      "16/16 [==============================] - 108s 7s/step - factorized_top_k/top_20_categorical_accuracy: 3.5128e-04 - loss: 31050.1298 - regularization_loss: 0.0000e+00 - total_loss: 31050.1298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 00:41:26,099] Trial 18 finished with value: 0.00035128058516420424 and parameters: {'window_size': 4, 'embedding_dimension': 128, 'dropout': 0.5, 'learning_rate': 0.00046017956058435884, 'batch_size': 256}. Best is trial 17 with value: 0.2700389623641968.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.02522, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.02522 to 0.05762, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05762 to 0.08518, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08518 to 0.10895, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.10895 to 0.13450, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13450 to 0.15424, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15424 to 0.16857, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16857 to 0.18198, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18198 to 0.19005, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19005 to 0.19594, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19594 to 0.19957, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19957 to 0.20257, saving model to gru_weights\\weights12.h5\n",
      "\n",
      "Epoch 13: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20257 to 0.20479, saving model to gru_weights\\weights13.h5\n",
      "Epoch 13: early stopping\n",
      "gru_weights\\weights13.h5\n",
      "7/7 [==============================] - 54s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.1993 - loss: 30796.8301 - regularization_loss: 0.0000e+00 - total_loss: 30796.8301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 01:10:59,071] Trial 19 finished with value: 0.19934822618961334 and parameters: {'window_size': 14, 'embedding_dimension': 256, 'dropout': 0.30000000000000004, 'learning_rate': 0.0262494124486439, 'batch_size': 256}. Best is trial 17 with value: 0.2700389623641968.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.01786, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01786\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.004504784941673279.\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01786\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0022523924708366394.\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01786\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0011261962354183197.\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights01.h5\n",
      "13/13 [==============================] - 77s 6s/step - factorized_top_k/top_20_categorical_accuracy: 0.0166 - loss: 30862.4601 - regularization_loss: 0.0000e+00 - total_loss: 30862.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 01:19:08,841] Trial 20 finished with value: 0.016597183421254158 and parameters: {'window_size': 6, 'embedding_dimension': 64, 'dropout': 0.65, 'learning_rate': 0.00900957005566355, 'batch_size': 256}. Best is trial 17 with value: 0.2700389623641968.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03470, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03470 to 0.08488, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08488 to 0.12906, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.12906 to 0.15998, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15998 to 0.17854, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17854 to 0.19065, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19065 to 0.20324, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20324 to 0.20672, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20672 to 0.21175, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21175 to 0.21372, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21372 to 0.21635, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21635 to 0.21724, saving model to gru_weights\\weights12.h5\n",
      "\n",
      "Epoch 13: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21724 to 0.21812, saving model to gru_weights\\weights13.h5\n",
      "Epoch 13: early stopping\n",
      "gru_weights\\weights13.h5\n",
      "7/7 [==============================] - 61s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2166 - loss: 30654.8857 - regularization_loss: 0.0000e+00 - total_loss: 30654.8857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 01:49:51,864] Trial 21 finished with value: 0.21656852960586548 and parameters: {'window_size': 14, 'embedding_dimension': 256, 'dropout': 0.30000000000000004, 'learning_rate': 0.031529149228342095, 'batch_size': 256}. Best is trial 17 with value: 0.2700389623641968.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03903, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03903 to 0.09010, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09010 to 0.14298, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14298 to 0.17850, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17850 to 0.20057, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20057 to 0.21527, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21527 to 0.22305, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.22305 to 0.22834, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.22834 to 0.23105, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23105 to 0.23146, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.23146\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.019550208002328873.\n",
      "Epoch 11: early stopping\n",
      "gru_weights\\weights10.h5\n",
      "7/7 [==============================] - 61s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2275 - loss: 29171.7935 - regularization_loss: 0.0000e+00 - total_loss: 29171.7935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 02:17:59,994] Trial 22 finished with value: 0.2274562120437622 and parameters: {'window_size': 14, 'embedding_dimension': 256, 'dropout': 0.2, 'learning_rate': 0.039100415252113774, 'batch_size': 256}. Best is trial 17 with value: 0.2700389623641968.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03204, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03204 to 0.07307, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07307 to 0.10227, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.10227 to 0.13686, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13686 to 0.15412, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15412 to 0.17376, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17376 to 0.18186, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18186 to 0.18932, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18932 to 0.19146, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19146 to 0.19592, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19592 to 0.19624, saving model to gru_weights\\weights11.h5\n",
      "Epoch 11: early stopping\n",
      "gru_weights\\weights11.h5\n",
      "7/7 [==============================] - 55s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.1972 - loss: 26911.7021 - regularization_loss: 0.0000e+00 - total_loss: 26911.7021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 02:45:57,684] Trial 23 finished with value: 0.1972133368253708 and parameters: {'window_size': 16, 'embedding_dimension': 256, 'dropout': 0.0, 'learning_rate': 0.03363070305334428, 'batch_size': 256}. Best is trial 17 with value: 0.2700389623641968.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.16635, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16635 to 0.24018, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24018 to 0.24936, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24936 to 0.26825, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26825 to 0.27087, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27087 to 0.27331, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27331 to 0.27346, saving model to gru_weights\\weights07.h5\n",
      "Epoch 7: early stopping\n",
      "gru_weights\\weights07.h5\n",
      "16/16 [==============================] - 118s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.2701 - loss: 26202.8688 - regularization_loss: 0.0000e+00 - total_loss: 26202.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 03:10:19,258] Trial 24 finished with value: 0.2700549364089966 and parameters: {'window_size': 4, 'embedding_dimension': 256, 'dropout': 0.2, 'learning_rate': 0.049772857853953335, 'batch_size': 64}. Best is trial 24 with value: 0.2700549364089966.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.01632, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01632\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0060929046012461185.\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01632\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0030464523006230593.\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01632\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0015232261503115296.\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights01.h5\n",
      "16/16 [==============================] - 93s 6s/step - factorized_top_k/top_20_categorical_accuracy: 0.0163 - loss: 30864.6017 - regularization_loss: 0.0000e+00 - total_loss: 30864.6017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 03:21:05,172] Trial 25 finished with value: 0.016318580135703087 and parameters: {'window_size': 4, 'embedding_dimension': 32, 'dropout': 0.2, 'learning_rate': 0.012185809661803701, 'batch_size': 256}. Best is trial 24 with value: 0.2700549364089966.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.10120, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.10120 to 0.17972, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17972 to 0.22453, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.22453 to 0.24398, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24398 to 0.25324, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25324 to 0.26105, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26105 to 0.26581, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26581 to 0.26677, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26677 to 0.26876, saving model to gru_weights\\weights09.h5\n",
      "Epoch 9: early stopping\n",
      "gru_weights\\weights09.h5\n",
      "13/13 [==============================] - 86s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.2722 - loss: 26078.4657 - regularization_loss: 0.0000e+00 - total_loss: 26078.4657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 03:41:06,981] Trial 26 finished with value: 0.2721623480319977 and parameters: {'window_size': 6, 'embedding_dimension': 128, 'dropout': 0.35000000000000003, 'learning_rate': 0.05867287609888114, 'batch_size': 64}. Best is trial 26 with value: 0.2721623480319977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.17107, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17107 to 0.24878, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24878 to 0.28642, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28642 to 0.30203, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.30203 to 0.30679, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.30679 to 0.30888, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.30888 to 0.30970, saving model to gru_weights\\weights07.h5\n",
      "Epoch 7: early stopping\n",
      "gru_weights\\weights07.h5\n",
      "13/13 [==============================] - 96s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.3075 - loss: 26213.2814 - regularization_loss: 0.0000e+00 - total_loss: 26213.2814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 04:03:01,685] Trial 27 finished with value: 0.3074805438518524 and parameters: {'window_size': 6, 'embedding_dimension': 256, 'dropout': 0.1, 'learning_rate': 0.09409465936263611, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03019, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03019 to 0.04885, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04885 to 0.06904, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06904 to 0.08560, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08560 to 0.09541, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09541 to 0.10358, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.10358 to 0.10969, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.10969 to 0.11714, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11714 to 0.12104, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.12104 to 0.12601, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.12601 to 0.12906, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.12906 to 0.13345, saving model to gru_weights\\weights12.h5\n",
      "\n",
      "Epoch 13: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13345 to 0.13598, saving model to gru_weights\\weights13.h5\n",
      "\n",
      "Epoch 14: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13598 to 0.13748, saving model to gru_weights\\weights14.h5\n",
      "Epoch 14: early stopping\n",
      "gru_weights\\weights14.h5\n",
      "13/13 [==============================] - 99s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.1356 - loss: 27278.3357 - regularization_loss: 0.0000e+00 - total_loss: 27278.3357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 04:44:25,248] Trial 28 finished with value: 0.13555023074150085 and parameters: {'window_size': 6, 'embedding_dimension': 256, 'dropout': 0.1, 'learning_rate': 0.01331797507431468, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.01647, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01647 to 0.04137, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04137 to 0.06098, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06098 to 0.07147, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07147 to 0.07747, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07747 to 0.08507, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08507 to 0.08949, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08949 to 0.09211, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.09211 to 0.09477, saving model to gru_weights\\weights09.h5\n",
      "Epoch 9: early stopping\n",
      "gru_weights\\weights09.h5\n",
      "10/10 [==============================] - 68s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.0943 - loss: 29279.7896 - regularization_loss: 0.0000e+00 - total_loss: 29279.7896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 05:01:36,877] Trial 29 finished with value: 0.09427767246961594 and parameters: {'window_size': 8, 'embedding_dimension': 128, 'dropout': 0.0, 'learning_rate': 0.016547085013397276, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.01117, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01117 to 0.01508, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01508\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.002096988493576646.\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.01508\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.001048494246788323.\n",
      "Epoch 4: early stopping\n",
      "gru_weights\\weights02.h5\n",
      "13/13 [==============================] - 83s 6s/step - factorized_top_k/top_20_categorical_accuracy: 0.0146 - loss: 30822.6233 - regularization_loss: 0.0000e+00 - total_loss: 30822.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 05:11:19,938] Trial 30 finished with value: 0.014611028134822845 and parameters: {'window_size': 6, 'embedding_dimension': 128, 'dropout': 0.15000000000000002, 'learning_rate': 0.004193976907936342, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.17827, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17827 to 0.24171, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24171 to 0.25358, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25358 to 0.26827, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26827 to 0.27165, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27165 to 0.27432, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.27432\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.02635345794260502.\n",
      "Epoch 7: early stopping\n",
      "gru_weights\\weights06.h5\n",
      "16/16 [==============================] - 119s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.2714 - loss: 26078.1594 - regularization_loss: 0.0000e+00 - total_loss: 26078.1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 05:35:18,618] Trial 31 finished with value: 0.27144408226013184 and parameters: {'window_size': 4, 'embedding_dimension': 256, 'dropout': 0.25, 'learning_rate': 0.05270691688705825, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.15347, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15347 to 0.24200, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24200 to 0.25501, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25501 to 0.28216, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28216 to 0.28583, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28583 to 0.28814, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28814 to 0.28856, saving model to gru_weights\\weights07.h5\n",
      "Epoch 7: early stopping\n",
      "gru_weights\\weights07.h5\n",
      "14/14 [==============================] - 109s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2839 - loss: 28772.9001 - regularization_loss: 0.0000e+00 - total_loss: 28772.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 05:58:48,252] Trial 32 finished with value: 0.28390905261039734 and parameters: {'window_size': 5, 'embedding_dimension': 256, 'dropout': 0.15000000000000002, 'learning_rate': 0.06523692448088197, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.06734, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06734 to 0.16262, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16262 to 0.20451, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20451 to 0.24197, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24197 to 0.25902, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25902 to 0.26819, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26819 to 0.27601, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27601 to 0.27690, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27690 to 0.27779, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.27779\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.023936301469802856.\n",
      "Epoch 10: early stopping\n",
      "gru_weights\\weights09.h5\n",
      "11/11 [==============================] - 86s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2807 - loss: 30284.5205 - regularization_loss: 0.0000e+00 - total_loss: 30284.5205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 06:26:53,141] Trial 33 finished with value: 0.2806849777698517 and parameters: {'window_size': 7, 'embedding_dimension': 256, 'dropout': 0.1, 'learning_rate': 0.04787260456427912, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.11796, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11796 to 0.21245, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21245 to 0.24866, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24866 to 0.27764, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27764 to 0.28633, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28633 to 0.29025, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29025 to 0.29051, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29051 to 0.29083, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29083 to 0.29132, saving model to gru_weights\\weights09.h5\n",
      "Epoch 9: early stopping\n",
      "gru_weights\\weights09.h5\n",
      "11/11 [==============================] - 87s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2890 - loss: 30095.7376 - regularization_loss: 0.0000e+00 - total_loss: 30095.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 06:52:18,591] Trial 34 finished with value: 0.28902480006217957 and parameters: {'window_size': 7, 'embedding_dimension': 256, 'dropout': 0.05, 'learning_rate': 0.09918895965149119, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.02969, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.02969 to 0.08024, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08024 to 0.14000, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14000 to 0.19133, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19133 to 0.21732, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21732 to 0.23240, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23240 to 0.23990, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23990 to 0.24370, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24370 to 0.24555, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24555 to 0.24661, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24661 to 0.24695, saving model to gru_weights\\weights11.h5\n",
      "Epoch 11: early stopping\n",
      "gru_weights\\weights11.h5\n",
      "11/11 [==============================] - 87s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2458 - loss: 33374.2637 - regularization_loss: 0.0000e+00 - total_loss: 33374.2637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 07:23:02,699] Trial 35 finished with value: 0.2457689344882965 and parameters: {'window_size': 7, 'embedding_dimension': 256, 'dropout': 0.05, 'learning_rate': 0.09514388917353493, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.08208, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08208 to 0.16764, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16764 to 0.21687, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21687 to 0.24781, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24781 to 0.25928, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25928 to 0.26597, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26597 to 0.26927, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26927 to 0.27205, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27205 to 0.27291, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27291 to 0.27296, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.02159002050757408.\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27296 to 0.27331, saving model to gru_weights\\weights11.h5\n",
      "Epoch 11: early stopping\n",
      "gru_weights\\weights11.h5\n",
      "10/10 [==============================] - 73s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.2723 - loss: 25292.6969 - regularization_loss: 0.0000e+00 - total_loss: 25292.6969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 07:52:31,099] Trial 36 finished with value: 0.2722610533237457 and parameters: {'window_size': 9, 'embedding_dimension': 256, 'dropout': 0.05, 'learning_rate': 0.043180041386325634, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03222, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03222 to 0.05432, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05432 to 0.07790, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07790 to 0.11559, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11559 to 0.14031, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14031 to 0.15896, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15896 to 0.16915, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16915 to 0.18088, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18088 to 0.18628, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18628 to 0.18996, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18996 to 0.19831, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19831 to 0.19848, saving model to gru_weights\\weights12.h5\n",
      "\n",
      "Epoch 13: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19848 to 0.20400, saving model to gru_weights\\weights13.h5\n",
      "\n",
      "Epoch 14: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20400 to 0.20534, saving model to gru_weights\\weights14.h5\n",
      "Epoch 14: early stopping\n",
      "gru_weights\\weights14.h5\n",
      "9/9 [==============================] - 68s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.2080 - loss: 27519.6510 - regularization_loss: 0.0000e+00 - total_loss: 27519.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 08:26:47,384] Trial 37 finished with value: 0.20796847343444824 and parameters: {'window_size': 10, 'embedding_dimension': 256, 'dropout': 0.1, 'learning_rate': 0.02119336158876659, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.15820, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15820 to 0.23454, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23454 to 0.25589, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25589 to 0.27875, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27875 to 0.28335, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28335 to 0.28745, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28745 to 0.28793, saving model to gru_weights\\weights07.h5\n",
      "Epoch 7: early stopping\n",
      "gru_weights\\weights07.h5\n",
      "14/14 [==============================] - 108s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2837 - loss: 27714.7783 - regularization_loss: 0.0000e+00 - total_loss: 27714.7783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 08:49:36,839] Trial 38 finished with value: 0.28367993235588074 and parameters: {'window_size': 5, 'embedding_dimension': 256, 'dropout': 0.15000000000000002, 'learning_rate': 0.04860160098819423, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.01458, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.01458 to 0.02800, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.02800 to 0.03493, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03493 to 0.04219, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04219 to 0.04727, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04727 to 0.05226, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05226 to 0.05661, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05661 to 0.05976, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05976 to 0.06186, saving model to gru_weights\\weights09.h5\n",
      "Epoch 9: early stopping\n",
      "gru_weights\\weights09.h5\n",
      "14/14 [==============================] - 133s 9s/step - factorized_top_k/top_20_categorical_accuracy: 0.0623 - loss: 29461.8827 - regularization_loss: 0.0000e+00 - total_loss: 29461.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 09:09:48,131] Trial 39 finished with value: 0.062266480177640915 and parameters: {'window_size': 5, 'embedding_dimension': 32, 'dropout': 0.15000000000000002, 'learning_rate': 0.025389384930428888, 'batch_size': 4096}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.06956, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06956 to 0.12020, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.12020 to 0.15696, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15696 to 0.17741, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17741 to 0.19140, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19140 to 0.19975, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19975 to 0.20430, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20430 to 0.20735, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20735 to 0.20853, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20853 to 0.21022, saving model to gru_weights\\weights10.h5\n",
      "Epoch 10: early stopping\n",
      "gru_weights\\weights10.h5\n",
      "14/14 [==============================] - 96s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.2050 - loss: 27134.2096 - regularization_loss: 0.0000e+00 - total_loss: 27134.2096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 09:34:33,348] Trial 40 finished with value: 0.20500528812408447 and parameters: {'window_size': 5, 'embedding_dimension': 64, 'dropout': 0.05, 'learning_rate': 0.05663098829186866, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.04388, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04388 to 0.11705, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11705 to 0.16873, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16873 to 0.20838, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20838 to 0.22735, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.22735 to 0.23899, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23899 to 0.24548, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24548 to 0.24650, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24650 to 0.24737, saving model to gru_weights\\weights09.h5\n",
      "Epoch 9: early stopping\n",
      "gru_weights\\weights09.h5\n",
      "11/11 [==============================] - 105s 9s/step - factorized_top_k/top_20_categorical_accuracy: 0.2489 - loss: 30662.7101 - regularization_loss: 0.0000e+00 - total_loss: 30662.7101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 10:04:08,582] Trial 41 finished with value: 0.2488602250814438 and parameters: {'window_size': 7, 'embedding_dimension': 256, 'dropout': 0.1, 'learning_rate': 0.047600488259356866, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.07835, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07835 to 0.17367, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17367 to 0.21686, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21686 to 0.25346, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25346 to 0.26269, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26269 to 0.26957, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26957 to 0.27210, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.27210 to 0.27281, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy did not improve from 0.27281\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.04837014898657799.\n",
      "Epoch 9: early stopping\n",
      "gru_weights\\weights08.h5\n",
      "11/11 [==============================] - 92s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2715 - loss: 30679.4176 - regularization_loss: 0.0000e+00 - total_loss: 30679.4176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 10:35:53,636] Trial 42 finished with value: 0.2714778184890747 and parameters: {'window_size': 7, 'embedding_dimension': 256, 'dropout': 0.0, 'learning_rate': 0.09674029488386075, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.10245, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.10245 to 0.17529, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17529 to 0.20196, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20196 to 0.22011, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.22011 to 0.23183, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23183 to 0.23750, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23750 to 0.24221, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24221 to 0.24418, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24418 to 0.24704, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.24704 to 0.24737, saving model to gru_weights\\weights10.h5\n",
      "Epoch 10: early stopping\n",
      "gru_weights\\weights10.h5\n",
      "14/14 [==============================] - 129s 9s/step - factorized_top_k/top_20_categorical_accuracy: 0.2459 - loss: 28658.6990 - regularization_loss: 0.0000e+00 - total_loss: 28658.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 11:09:57,474] Trial 43 finished with value: 0.245858296751976 and parameters: {'window_size': 5, 'embedding_dimension': 256, 'dropout': 0.15000000000000002, 'learning_rate': 0.031241644878902044, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.05150, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05150 to 0.12640, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.12640 to 0.16659, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16659 to 0.21480, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21480 to 0.23771, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.23771 to 0.25072, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25072 to 0.25706, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25706 to 0.25949, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.25949 to 0.26049, saving model to gru_weights\\weights09.h5\n",
      "Epoch 9: early stopping\n",
      "gru_weights\\weights09.h5\n",
      "10/10 [==============================] - 72s 7s/step - factorized_top_k/top_20_categorical_accuracy: 0.2590 - loss: 26443.7671 - regularization_loss: 0.0000e+00 - total_loss: 26443.7671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 11:35:34,981] Trial 44 finished with value: 0.2590353190898895 and parameters: {'window_size': 9, 'embedding_dimension': 256, 'dropout': 0.1, 'learning_rate': 0.05953931504574247, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.02490, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.02490 to 0.04911, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04911 to 0.07730, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07730 to 0.11096, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11096 to 0.13736, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13736 to 0.15482, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15482 to 0.17211, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.17211 to 0.18388, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18388 to 0.19426, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.19426 to 0.20165, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20165 to 0.20753, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.20753 to 0.21125, saving model to gru_weights\\weights12.h5\n",
      "\n",
      "Epoch 13: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21125 to 0.21269, saving model to gru_weights\\weights13.h5\n",
      "\n",
      "Epoch 14: val_factorized_top_k/top_20_categorical_accuracy improved from 0.21269 to 0.21431, saving model to gru_weights\\weights14.h5\n",
      "Epoch 14: early stopping\n",
      "gru_weights\\weights14.h5\n",
      "8/8 [==============================] - 85s 10s/step - factorized_top_k/top_20_categorical_accuracy: 0.2116 - loss: 33267.8108 - regularization_loss: 0.0000e+00 - total_loss: 33267.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 12:46:44,996] Trial 45 finished with value: 0.21159321069717407 and parameters: {'window_size': 11, 'embedding_dimension': 512, 'dropout': 0.05, 'learning_rate': 0.016616070598016213, 'batch_size': 256}. Best is trial 27 with value: 0.3074805438518524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.18029, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18029 to 0.26288, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26288 to 0.28280, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28280 to 0.30719, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.30719 to 0.31259, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.31259 to 0.31505, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.31505 to 0.31617, saving model to gru_weights\\weights07.h5\n",
      "Epoch 7: early stopping\n",
      "gru_weights\\weights07.h5\n",
      "13/13 [==============================] - 108s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.3162 - loss: 25323.2041 - regularization_loss: 0.0000e+00 - total_loss: 25323.2041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 13:09:12,130] Trial 46 finished with value: 0.316192090511322 and parameters: {'window_size': 6, 'embedding_dimension': 256, 'dropout': 0.25, 'learning_rate': 0.06258085768442055, 'batch_size': 256}. Best is trial 46 with value: 0.316192090511322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.04727, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04727 to 0.07103, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07103 to 0.08784, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.08784 to 0.10275, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.10275 to 0.11606, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.11606 to 0.12444, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.12444 to 0.13427, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.13427 to 0.14104, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14104 to 0.14731, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.14731 to 0.15311, saving model to gru_weights\\weights10.h5\n",
      "\n",
      "Epoch 11: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15311 to 0.15641, saving model to gru_weights\\weights11.h5\n",
      "\n",
      "Epoch 12: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15641 to 0.15923, saving model to gru_weights\\weights12.h5\n",
      "\n",
      "Epoch 13: val_factorized_top_k/top_20_categorical_accuracy improved from 0.15923 to 0.16245, saving model to gru_weights\\weights13.h5\n",
      "\n",
      "Epoch 14: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16245 to 0.16418, saving model to gru_weights\\weights14.h5\n",
      "\n",
      "Epoch 15: val_factorized_top_k/top_20_categorical_accuracy improved from 0.16418 to 0.16550, saving model to gru_weights\\weights15.h5\n",
      "Epoch 15: early stopping\n",
      "gru_weights\\weights15.h5\n",
      "13/13 [==============================] - 78s 6s/step - factorized_top_k/top_20_categorical_accuracy: 0.1638 - loss: 25922.6763 - regularization_loss: 0.0000e+00 - total_loss: 25922.6763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 13:36:02,478] Trial 47 finished with value: 0.16382835805416107 and parameters: {'window_size': 6, 'embedding_dimension': 32, 'dropout': 0.25, 'learning_rate': 0.0661464695454943, 'batch_size': 256}. Best is trial 46 with value: 0.316192090511322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.03231, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.03231 to 0.04360, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.04360 to 0.05034, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05034 to 0.05791, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.05791 to 0.06093, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06093 to 0.06663, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06663 to 0.06853, saving model to gru_weights\\weights07.h5\n",
      "\n",
      "Epoch 8: val_factorized_top_k/top_20_categorical_accuracy improved from 0.06853 to 0.07194, saving model to gru_weights\\weights08.h5\n",
      "\n",
      "Epoch 9: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07194 to 0.07376, saving model to gru_weights\\weights09.h5\n",
      "\n",
      "Epoch 10: val_factorized_top_k/top_20_categorical_accuracy improved from 0.07376 to 0.07544, saving model to gru_weights\\weights10.h5\n",
      "Epoch 10: early stopping\n",
      "gru_weights\\weights10.h5\n",
      "10/10 [==============================] - 63s 6s/step - factorized_top_k/top_20_categorical_accuracy: 0.0760 - loss: 29437.0096 - regularization_loss: 0.0000e+00 - total_loss: 29437.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 13:51:11,326] Trial 48 finished with value: 0.07596030086278915 and parameters: {'window_size': 8, 'embedding_dimension': 64, 'dropout': 0.2, 'learning_rate': 0.02217262658073085, 'batch_size': 256}. Best is trial 46 with value: 0.316192090511322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_factorized_top_k/top_20_categorical_accuracy improved from -inf to 0.18717, saving model to gru_weights\\weights01.h5\n",
      "\n",
      "Epoch 2: val_factorized_top_k/top_20_categorical_accuracy improved from 0.18717 to 0.26139, saving model to gru_weights\\weights02.h5\n",
      "\n",
      "Epoch 3: val_factorized_top_k/top_20_categorical_accuracy improved from 0.26139 to 0.28239, saving model to gru_weights\\weights03.h5\n",
      "\n",
      "Epoch 4: val_factorized_top_k/top_20_categorical_accuracy improved from 0.28239 to 0.29336, saving model to gru_weights\\weights04.h5\n",
      "\n",
      "Epoch 5: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29336 to 0.29685, saving model to gru_weights\\weights05.h5\n",
      "\n",
      "Epoch 6: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29685 to 0.29822, saving model to gru_weights\\weights06.h5\n",
      "\n",
      "Epoch 7: val_factorized_top_k/top_20_categorical_accuracy improved from 0.29822 to 0.29891, saving model to gru_weights\\weights07.h5\n",
      "Epoch 7: early stopping\n",
      "gru_weights\\weights07.h5\n",
      "14/14 [==============================] - 110s 8s/step - factorized_top_k/top_20_categorical_accuracy: 0.2948 - loss: 28824.8063 - regularization_loss: 0.0000e+00 - total_loss: 28824.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-07 14:14:29,691] Trial 49 finished with value: 0.2947655916213989 and parameters: {'window_size': 5, 'embedding_dimension': 256, 'dropout': 0.15000000000000002, 'learning_rate': 0.09612145478087826, 'batch_size': 256}. Best is trial 46 with value: 0.316192090511322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'window_size': 6, 'embedding_dimension': 256, 'dropout': 0.25, 'learning_rate': 0.06258085768442055, 'batch_size': 256}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "\n",
    "    df = (pd\n",
    "            .read_csv(r'retailrocket_data\\events.csv')\n",
    "            .pipe(lambda df: df.assign(timestamp = pd.to_datetime(df['timestamp'] * 1000000)))\n",
    "            .sort_values(['visitorid', 'timestamp'])\n",
    "            .set_index(['visitorid', 'timestamp'])\n",
    "            .pipe(lambda df: df.assign(itemid_shift = df.groupby(level=0, as_index=False)['itemid'].shift()))\n",
    "            .reset_index()    \n",
    "             # drop duplicates and videos with 0 watch duration\n",
    "            .query(\"itemid_shift != itemid\")\n",
    "          )\n",
    "\n",
    "    counts = df.visitorid.value_counts()\n",
    "    max_limit = counts[counts<500].index.to_list()\n",
    "    min_limit = counts[counts>5].index.to_list()\n",
    "\n",
    "    df = (df\n",
    "            .query('visitorid in @min_limit and visitorid in @max_limit')\n",
    "            .rename(columns={'visitorid':'clientid', 'itemid':'item_id'})\n",
    "            .astype(str)\n",
    "          )[['clientid', 'item_id']]\n",
    "\n",
    "\n",
    "    # group by user\n",
    "    df_group = df.groupby(\"clientid\")\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "                         data={\n",
    "                               \"clientid\": list(df_group.groups.keys()),\n",
    "                               \"item_id\": list(df_group.item_id.apply(list)),\n",
    "                               }\n",
    "                        )\n",
    "\n",
    "    # create a sequences with length of n for each user\n",
    "    data.item_id = data.item_id.apply(lambda ids: create_sequences(ids, window_size=trial.suggest_int('window_size', 4, 20), step_size=1))\n",
    "\n",
    "    # explode\n",
    "    data_transformed = data[[\"clientid\", \"item_id\"]].explode([\"item_id\"], ignore_index=True)\n",
    "\n",
    "    # target is the last records (item) in the list  -> item seq:[4,5,6,7,8] item target:[9]\n",
    "    # convert each variable to numpy array\n",
    "    data_transformed['target'] = data_transformed['item_id'].apply(lambda x : np.array(x[-1], dtype=str))\n",
    "    data_transformed['item_id'] = data_transformed['item_id'].apply(lambda x : np.array(x[:-1], dtype=str))\n",
    "    data_transformed = data_transformed[['item_id', 'target']]\n",
    "\n",
    "    # convert each variable to tensor and store in a dict then convert to tensorflow Dataset\n",
    "    data_transformed = {\n",
    "                        'item_id': tf.constant(data_transformed[\"item_id\"].to_list(), name='item_id'),\n",
    "                        'target': tf.constant(data_transformed[\"target\"], name='target')\n",
    "                       }\n",
    "\n",
    "    tf_dict = tf.data.Dataset.from_tensor_slices(data_transformed)\n",
    "\n",
    "    # map rows to a dictionary\n",
    "    data = tf_dict.map(lambda x: {'item_id': x['item_id'],  'target': x['target']})\n",
    "    items = tf.data.Dataset.from_tensor_slices({'item_id':df.item_id.unique()}).map(lambda x: x['item_id'])\n",
    "    \n",
    "    # Split the dataset to train validation and test\n",
    "    cached_train, cached_validation, cached_test = train_validation_test_split(data, len(data_transformed['target']), 0.15, 0.15)\n",
    "\n",
    "    # Train The model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = GRU4REC(items=items, \n",
    "                    embedding_dimension=trial.suggest_categorical('embedding_dimension', [32, 64, 128, 256, 512]),\n",
    "                    dropout=trial.suggest_float('dropout', 0, 0.7, step=0.05), \n",
    "                    recurrent_dropout=trial.suggest_float('dropout', 0, 0.7, step=0.05)\n",
    "                   )\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)))\n",
    "\n",
    "    # Callbacks setting\n",
    "    es = EarlyStopping(monitor='val_factorized_top_k/top_20_categorical_accuracy', min_delta=0.01, patience=3, verbose=1)\n",
    "    mcp = ModelCheckpoint(filepath=r'gru_weights/weights{epoch:02d}.h5', monitor='val_factorized_top_k/top_20_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    rlr = ReduceLROnPlateau(monitor='val_factorized_top_k/top_20_categorical_accuracy', factor=0.5, patience=1, min_lr=1e-7, verbose=1)\n",
    "\n",
    "    history = model.fit(cached_train, batch_size=trial.suggest_categorical('batch_size', [32, 64, 128, 256, 256, 1024, 4096, 16384]),\n",
    "                        epochs=30,\n",
    "                        validation_data=cached_validation, \n",
    "                        callbacks=[es, mcp, rlr],\n",
    "                        verbose=0)\n",
    "    # Save and load the best model \n",
    "    list_of_files = glob.glob('gru_weights/*')\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "\n",
    "    model.built = True\n",
    "    model.load_weights(latest_file)\n",
    "\n",
    "    for f in list_of_files:\n",
    "        os.remove(f)\n",
    "    # Metric to optimize: top 20 accuracy\n",
    "    evaluate = model.evaluate(cached_test)\n",
    "\n",
    "    return evaluate[0]\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1234))\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best hyperparameters from the study\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0efdfbb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_batch_size</th>\n",
       "      <th>params_dropout</th>\n",
       "      <th>params_embedding_dimension</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_window_size</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.097298</td>\n",
       "      <td>2023-12-06 16:49:22.759134</td>\n",
       "      <td>2023-12-06 17:16:26.362385</td>\n",
       "      <td>0 days 00:27:03.603251</td>\n",
       "      <td>32</td>\n",
       "      <td>0.20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>2023-12-06 17:16:26.367383</td>\n",
       "      <td>2023-12-06 17:26:17.915312</td>\n",
       "      <td>0 days 00:09:51.547929</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>12</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>2023-12-06 17:26:17.965282</td>\n",
       "      <td>2023-12-06 17:56:44.608557</td>\n",
       "      <td>0 days 00:30:26.643275</td>\n",
       "      <td>256</td>\n",
       "      <td>0.30</td>\n",
       "      <td>512</td>\n",
       "      <td>0.043377</td>\n",
       "      <td>17</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.244595</td>\n",
       "      <td>2023-12-06 17:56:44.634541</td>\n",
       "      <td>2023-12-06 18:22:35.287596</td>\n",
       "      <td>0 days 00:25:50.653055</td>\n",
       "      <td>64</td>\n",
       "      <td>0.45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.044513</td>\n",
       "      <td>9</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>2023-12-06 18:22:35.292590</td>\n",
       "      <td>2023-12-06 18:37:22.253953</td>\n",
       "      <td>0 days 00:14:46.961363</td>\n",
       "      <td>256</td>\n",
       "      <td>0.55</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>10</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.066318</td>\n",
       "      <td>2023-12-06 18:37:22.257952</td>\n",
       "      <td>2023-12-06 18:54:04.093634</td>\n",
       "      <td>0 days 00:16:41.835682</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.55</td>\n",
       "      <td>64</td>\n",
       "      <td>0.021088</td>\n",
       "      <td>11</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>2023-12-06 18:54:04.100631</td>\n",
       "      <td>2023-12-06 18:59:38.453849</td>\n",
       "      <td>0 days 00:05:34.353218</td>\n",
       "      <td>32</td>\n",
       "      <td>0.55</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>13</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.019051</td>\n",
       "      <td>2023-12-06 18:59:38.455849</td>\n",
       "      <td>2023-12-06 19:05:35.622150</td>\n",
       "      <td>0 days 00:05:57.166301</td>\n",
       "      <td>64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>32</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>11</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>2023-12-06 19:05:35.624148</td>\n",
       "      <td>2023-12-06 19:21:12.219610</td>\n",
       "      <td>0 days 00:15:36.595462</td>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.017307</td>\n",
       "      <td>2023-12-06 19:21:12.222607</td>\n",
       "      <td>2023-12-06 19:31:02.540177</td>\n",
       "      <td>0 days 00:09:50.317570</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.70</td>\n",
       "      <td>64</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>2023-12-06 19:31:02.541175</td>\n",
       "      <td>2023-12-06 20:25:01.600839</td>\n",
       "      <td>0 days 00:53:59.059664</td>\n",
       "      <td>128</td>\n",
       "      <td>0.40</td>\n",
       "      <td>512</td>\n",
       "      <td>0.086243</td>\n",
       "      <td>19</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>2023-12-06 20:25:01.632245</td>\n",
       "      <td>2023-12-06 20:47:41.321119</td>\n",
       "      <td>0 days 00:22:39.688874</td>\n",
       "      <td>128</td>\n",
       "      <td>0.40</td>\n",
       "      <td>512</td>\n",
       "      <td>0.077809</td>\n",
       "      <td>20</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>2023-12-06 20:47:41.355100</td>\n",
       "      <td>2023-12-06 22:09:13.166440</td>\n",
       "      <td>0 days 01:21:31.811340</td>\n",
       "      <td>64</td>\n",
       "      <td>0.40</td>\n",
       "      <td>512</td>\n",
       "      <td>0.071860</td>\n",
       "      <td>16</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.194628</td>\n",
       "      <td>2023-12-06 22:09:13.207417</td>\n",
       "      <td>2023-12-06 22:35:36.344985</td>\n",
       "      <td>0 days 00:26:23.137568</td>\n",
       "      <td>128</td>\n",
       "      <td>0.30</td>\n",
       "      <td>256</td>\n",
       "      <td>0.095726</td>\n",
       "      <td>8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.185494</td>\n",
       "      <td>2023-12-06 22:35:36.346984</td>\n",
       "      <td>2023-12-06 23:14:02.949496</td>\n",
       "      <td>0 days 00:38:26.602512</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.018442</td>\n",
       "      <td>8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.073657</td>\n",
       "      <td>2023-12-06 23:14:02.965034</td>\n",
       "      <td>2023-12-06 23:44:23.613629</td>\n",
       "      <td>0 days 00:30:20.648595</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.15</td>\n",
       "      <td>256</td>\n",
       "      <td>0.008182</td>\n",
       "      <td>7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.170558</td>\n",
       "      <td>2023-12-06 23:44:23.615628</td>\n",
       "      <td>2023-12-07 00:05:53.691915</td>\n",
       "      <td>0 days 00:21:30.076287</td>\n",
       "      <td>256</td>\n",
       "      <td>0.45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.028616</td>\n",
       "      <td>9</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.270039</td>\n",
       "      <td>2023-12-07 00:05:53.692913</td>\n",
       "      <td>2023-12-07 00:29:42.984877</td>\n",
       "      <td>0 days 00:23:49.291964</td>\n",
       "      <td>256</td>\n",
       "      <td>0.30</td>\n",
       "      <td>256</td>\n",
       "      <td>0.087487</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>2023-12-07 00:29:42.985874</td>\n",
       "      <td>2023-12-07 00:41:26.098379</td>\n",
       "      <td>0 days 00:11:43.112505</td>\n",
       "      <td>256</td>\n",
       "      <td>0.50</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.199348</td>\n",
       "      <td>2023-12-07 00:41:26.100377</td>\n",
       "      <td>2023-12-07 01:10:59.070468</td>\n",
       "      <td>0 days 00:29:32.970091</td>\n",
       "      <td>256</td>\n",
       "      <td>0.30</td>\n",
       "      <td>256</td>\n",
       "      <td>0.026249</td>\n",
       "      <td>14</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.016597</td>\n",
       "      <td>2023-12-07 01:10:59.072467</td>\n",
       "      <td>2023-12-07 01:19:08.840372</td>\n",
       "      <td>0 days 00:08:09.767905</td>\n",
       "      <td>256</td>\n",
       "      <td>0.65</td>\n",
       "      <td>64</td>\n",
       "      <td>0.009010</td>\n",
       "      <td>6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.216569</td>\n",
       "      <td>2023-12-07 01:19:08.846370</td>\n",
       "      <td>2023-12-07 01:49:51.863787</td>\n",
       "      <td>0 days 00:30:43.017417</td>\n",
       "      <td>256</td>\n",
       "      <td>0.30</td>\n",
       "      <td>256</td>\n",
       "      <td>0.031529</td>\n",
       "      <td>14</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.227456</td>\n",
       "      <td>2023-12-07 01:49:51.865786</td>\n",
       "      <td>2023-12-07 02:17:59.993908</td>\n",
       "      <td>0 days 00:28:08.128122</td>\n",
       "      <td>256</td>\n",
       "      <td>0.20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>14</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.197213</td>\n",
       "      <td>2023-12-07 02:17:59.996907</td>\n",
       "      <td>2023-12-07 02:45:57.684431</td>\n",
       "      <td>0 days 00:27:57.687524</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00</td>\n",
       "      <td>256</td>\n",
       "      <td>0.033631</td>\n",
       "      <td>16</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.270055</td>\n",
       "      <td>2023-12-07 02:45:57.685422</td>\n",
       "      <td>2023-12-07 03:10:19.258210</td>\n",
       "      <td>0 days 00:24:21.572788</td>\n",
       "      <td>64</td>\n",
       "      <td>0.20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.016319</td>\n",
       "      <td>2023-12-07 03:10:19.259210</td>\n",
       "      <td>2023-12-07 03:21:05.165387</td>\n",
       "      <td>0 days 00:10:45.906177</td>\n",
       "      <td>256</td>\n",
       "      <td>0.20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.272162</td>\n",
       "      <td>2023-12-07 03:21:05.181578</td>\n",
       "      <td>2023-12-07 03:41:06.981124</td>\n",
       "      <td>0 days 00:20:01.799546</td>\n",
       "      <td>64</td>\n",
       "      <td>0.35</td>\n",
       "      <td>128</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.307481</td>\n",
       "      <td>2023-12-07 03:41:06.982122</td>\n",
       "      <td>2023-12-07 04:03:01.684526</td>\n",
       "      <td>0 days 00:21:54.702404</td>\n",
       "      <td>256</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.094095</td>\n",
       "      <td>6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.135550</td>\n",
       "      <td>2023-12-07 04:03:01.686516</td>\n",
       "      <td>2023-12-07 04:44:25.247756</td>\n",
       "      <td>0 days 00:41:23.561240</td>\n",
       "      <td>256</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.013318</td>\n",
       "      <td>6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.094278</td>\n",
       "      <td>2023-12-07 04:44:25.249754</td>\n",
       "      <td>2023-12-07 05:01:36.877506</td>\n",
       "      <td>0 days 00:17:11.627752</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00</td>\n",
       "      <td>128</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>2023-12-07 05:01:36.878505</td>\n",
       "      <td>2023-12-07 05:11:19.938416</td>\n",
       "      <td>0 days 00:09:43.059911</td>\n",
       "      <td>256</td>\n",
       "      <td>0.15</td>\n",
       "      <td>128</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.271444</td>\n",
       "      <td>2023-12-07 05:11:19.939418</td>\n",
       "      <td>2023-12-07 05:35:18.617812</td>\n",
       "      <td>0 days 00:23:58.678394</td>\n",
       "      <td>256</td>\n",
       "      <td>0.25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.052707</td>\n",
       "      <td>4</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.283909</td>\n",
       "      <td>2023-12-07 05:35:18.619813</td>\n",
       "      <td>2023-12-07 05:58:48.251572</td>\n",
       "      <td>0 days 00:23:29.631759</td>\n",
       "      <td>256</td>\n",
       "      <td>0.15</td>\n",
       "      <td>256</td>\n",
       "      <td>0.065237</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.280685</td>\n",
       "      <td>2023-12-07 05:58:48.253572</td>\n",
       "      <td>2023-12-07 06:26:53.140380</td>\n",
       "      <td>0 days 00:28:04.886808</td>\n",
       "      <td>256</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.047873</td>\n",
       "      <td>7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.289025</td>\n",
       "      <td>2023-12-07 06:26:53.141378</td>\n",
       "      <td>2023-12-07 06:52:18.591855</td>\n",
       "      <td>0 days 00:25:25.450477</td>\n",
       "      <td>256</td>\n",
       "      <td>0.05</td>\n",
       "      <td>256</td>\n",
       "      <td>0.099189</td>\n",
       "      <td>7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.245769</td>\n",
       "      <td>2023-12-07 06:52:18.592855</td>\n",
       "      <td>2023-12-07 07:23:02.698621</td>\n",
       "      <td>0 days 00:30:44.105766</td>\n",
       "      <td>256</td>\n",
       "      <td>0.05</td>\n",
       "      <td>256</td>\n",
       "      <td>0.095144</td>\n",
       "      <td>7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.272261</td>\n",
       "      <td>2023-12-07 07:23:02.700618</td>\n",
       "      <td>2023-12-07 07:52:31.090222</td>\n",
       "      <td>0 days 00:29:28.389604</td>\n",
       "      <td>256</td>\n",
       "      <td>0.05</td>\n",
       "      <td>256</td>\n",
       "      <td>0.043180</td>\n",
       "      <td>9</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.207968</td>\n",
       "      <td>2023-12-07 07:52:31.112218</td>\n",
       "      <td>2023-12-07 08:26:47.383104</td>\n",
       "      <td>0 days 00:34:16.270886</td>\n",
       "      <td>256</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>10</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.283680</td>\n",
       "      <td>2023-12-07 08:26:47.386098</td>\n",
       "      <td>2023-12-07 08:49:36.838285</td>\n",
       "      <td>0 days 00:22:49.452187</td>\n",
       "      <td>256</td>\n",
       "      <td>0.15</td>\n",
       "      <td>256</td>\n",
       "      <td>0.048602</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.062266</td>\n",
       "      <td>2023-12-07 08:49:36.841284</td>\n",
       "      <td>2023-12-07 09:09:48.129508</td>\n",
       "      <td>0 days 00:20:11.288224</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.15</td>\n",
       "      <td>32</td>\n",
       "      <td>0.025389</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.205005</td>\n",
       "      <td>2023-12-07 09:09:48.135505</td>\n",
       "      <td>2023-12-07 09:34:33.347694</td>\n",
       "      <td>0 days 00:24:45.212189</td>\n",
       "      <td>256</td>\n",
       "      <td>0.05</td>\n",
       "      <td>64</td>\n",
       "      <td>0.056631</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.248860</td>\n",
       "      <td>2023-12-07 09:34:33.349692</td>\n",
       "      <td>2023-12-07 10:04:08.582698</td>\n",
       "      <td>0 days 00:29:35.233006</td>\n",
       "      <td>256</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.271478</td>\n",
       "      <td>2023-12-07 10:04:08.584698</td>\n",
       "      <td>2023-12-07 10:35:53.635792</td>\n",
       "      <td>0 days 00:31:45.051094</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00</td>\n",
       "      <td>256</td>\n",
       "      <td>0.096740</td>\n",
       "      <td>7</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.245858</td>\n",
       "      <td>2023-12-07 10:35:53.638572</td>\n",
       "      <td>2023-12-07 11:09:57.470926</td>\n",
       "      <td>0 days 00:34:03.832354</td>\n",
       "      <td>256</td>\n",
       "      <td>0.15</td>\n",
       "      <td>256</td>\n",
       "      <td>0.031242</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.259035</td>\n",
       "      <td>2023-12-07 11:09:57.476922</td>\n",
       "      <td>2023-12-07 11:35:34.981873</td>\n",
       "      <td>0 days 00:25:37.504951</td>\n",
       "      <td>256</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.059539</td>\n",
       "      <td>9</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.211593</td>\n",
       "      <td>2023-12-07 11:35:34.982852</td>\n",
       "      <td>2023-12-07 12:46:44.991737</td>\n",
       "      <td>0 days 01:11:10.008885</td>\n",
       "      <td>256</td>\n",
       "      <td>0.05</td>\n",
       "      <td>512</td>\n",
       "      <td>0.016616</td>\n",
       "      <td>11</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.316192</td>\n",
       "      <td>2023-12-07 12:46:45.010977</td>\n",
       "      <td>2023-12-07 13:09:12.130360</td>\n",
       "      <td>0 days 00:22:27.119383</td>\n",
       "      <td>256</td>\n",
       "      <td>0.25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.062581</td>\n",
       "      <td>6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.163828</td>\n",
       "      <td>2023-12-07 13:09:12.131357</td>\n",
       "      <td>2023-12-07 13:36:02.469030</td>\n",
       "      <td>0 days 00:26:50.337673</td>\n",
       "      <td>256</td>\n",
       "      <td>0.25</td>\n",
       "      <td>32</td>\n",
       "      <td>0.066146</td>\n",
       "      <td>6</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.075960</td>\n",
       "      <td>2023-12-07 13:36:02.493059</td>\n",
       "      <td>2023-12-07 13:51:11.326515</td>\n",
       "      <td>0 days 00:15:08.833456</td>\n",
       "      <td>256</td>\n",
       "      <td>0.20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.022173</td>\n",
       "      <td>8</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.294766</td>\n",
       "      <td>2023-12-07 13:51:11.327507</td>\n",
       "      <td>2023-12-07 14:14:29.690376</td>\n",
       "      <td>0 days 00:23:18.362869</td>\n",
       "      <td>256</td>\n",
       "      <td>0.15</td>\n",
       "      <td>256</td>\n",
       "      <td>0.096121</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "0        0  0.097298 2023-12-06 16:49:22.759134 2023-12-06 17:16:26.362385   \n",
       "1        1  0.000366 2023-12-06 17:16:26.367383 2023-12-06 17:26:17.915312   \n",
       "2        2  0.010662 2023-12-06 17:26:17.965282 2023-12-06 17:56:44.608557   \n",
       "3        3  0.244595 2023-12-06 17:56:44.634541 2023-12-06 18:22:35.287596   \n",
       "4        4  0.017776 2023-12-06 18:22:35.292590 2023-12-06 18:37:22.253953   \n",
       "5        5  0.066318 2023-12-06 18:37:22.257952 2023-12-06 18:54:04.093634   \n",
       "6        6  0.005386 2023-12-06 18:54:04.100631 2023-12-06 18:59:38.453849   \n",
       "7        7  0.019051 2023-12-06 18:59:38.455849 2023-12-06 19:05:35.622150   \n",
       "8        8  0.000264 2023-12-06 19:05:35.624148 2023-12-06 19:21:12.219610   \n",
       "9        9  0.017307 2023-12-06 19:21:12.222607 2023-12-06 19:31:02.540177   \n",
       "10      10  0.147402 2023-12-06 19:31:02.541175 2023-12-06 20:25:01.600839   \n",
       "11      11  0.011544 2023-12-06 20:25:01.632245 2023-12-06 20:47:41.321119   \n",
       "12      12  0.086800 2023-12-06 20:47:41.355100 2023-12-06 22:09:13.166440   \n",
       "13      13  0.194628 2023-12-06 22:09:13.207417 2023-12-06 22:35:36.344985   \n",
       "14      14  0.185494 2023-12-06 22:35:36.346984 2023-12-06 23:14:02.949496   \n",
       "15      15  0.073657 2023-12-06 23:14:02.965034 2023-12-06 23:44:23.613629   \n",
       "16      16  0.170558 2023-12-06 23:44:23.615628 2023-12-07 00:05:53.691915   \n",
       "17      17  0.270039 2023-12-07 00:05:53.692913 2023-12-07 00:29:42.984877   \n",
       "18      18  0.000351 2023-12-07 00:29:42.985874 2023-12-07 00:41:26.098379   \n",
       "19      19  0.199348 2023-12-07 00:41:26.100377 2023-12-07 01:10:59.070468   \n",
       "20      20  0.016597 2023-12-07 01:10:59.072467 2023-12-07 01:19:08.840372   \n",
       "21      21  0.216569 2023-12-07 01:19:08.846370 2023-12-07 01:49:51.863787   \n",
       "22      22  0.227456 2023-12-07 01:49:51.865786 2023-12-07 02:17:59.993908   \n",
       "23      23  0.197213 2023-12-07 02:17:59.996907 2023-12-07 02:45:57.684431   \n",
       "24      24  0.270055 2023-12-07 02:45:57.685422 2023-12-07 03:10:19.258210   \n",
       "25      25  0.016319 2023-12-07 03:10:19.259210 2023-12-07 03:21:05.165387   \n",
       "26      26  0.272162 2023-12-07 03:21:05.181578 2023-12-07 03:41:06.981124   \n",
       "27      27  0.307481 2023-12-07 03:41:06.982122 2023-12-07 04:03:01.684526   \n",
       "28      28  0.135550 2023-12-07 04:03:01.686516 2023-12-07 04:44:25.247756   \n",
       "29      29  0.094278 2023-12-07 04:44:25.249754 2023-12-07 05:01:36.877506   \n",
       "30      30  0.014611 2023-12-07 05:01:36.878505 2023-12-07 05:11:19.938416   \n",
       "31      31  0.271444 2023-12-07 05:11:19.939418 2023-12-07 05:35:18.617812   \n",
       "32      32  0.283909 2023-12-07 05:35:18.619813 2023-12-07 05:58:48.251572   \n",
       "33      33  0.280685 2023-12-07 05:58:48.253572 2023-12-07 06:26:53.140380   \n",
       "34      34  0.289025 2023-12-07 06:26:53.141378 2023-12-07 06:52:18.591855   \n",
       "35      35  0.245769 2023-12-07 06:52:18.592855 2023-12-07 07:23:02.698621   \n",
       "36      36  0.272261 2023-12-07 07:23:02.700618 2023-12-07 07:52:31.090222   \n",
       "37      37  0.207968 2023-12-07 07:52:31.112218 2023-12-07 08:26:47.383104   \n",
       "38      38  0.283680 2023-12-07 08:26:47.386098 2023-12-07 08:49:36.838285   \n",
       "39      39  0.062266 2023-12-07 08:49:36.841284 2023-12-07 09:09:48.129508   \n",
       "40      40  0.205005 2023-12-07 09:09:48.135505 2023-12-07 09:34:33.347694   \n",
       "41      41  0.248860 2023-12-07 09:34:33.349692 2023-12-07 10:04:08.582698   \n",
       "42      42  0.271478 2023-12-07 10:04:08.584698 2023-12-07 10:35:53.635792   \n",
       "43      43  0.245858 2023-12-07 10:35:53.638572 2023-12-07 11:09:57.470926   \n",
       "44      44  0.259035 2023-12-07 11:09:57.476922 2023-12-07 11:35:34.981873   \n",
       "45      45  0.211593 2023-12-07 11:35:34.982852 2023-12-07 12:46:44.991737   \n",
       "46      46  0.316192 2023-12-07 12:46:45.010977 2023-12-07 13:09:12.130360   \n",
       "47      47  0.163828 2023-12-07 13:09:12.131357 2023-12-07 13:36:02.469030   \n",
       "48      48  0.075960 2023-12-07 13:36:02.493059 2023-12-07 13:51:11.326515   \n",
       "49      49  0.294766 2023-12-07 13:51:11.327507 2023-12-07 14:14:29.690376   \n",
       "\n",
       "                 duration  params_batch_size  params_dropout  \\\n",
       "0  0 days 00:27:03.603251                 32            0.20   \n",
       "1  0 days 00:09:51.547929                 32            0.05   \n",
       "2  0 days 00:30:26.643275                256            0.30   \n",
       "3  0 days 00:25:50.653055                 64            0.45   \n",
       "4  0 days 00:14:46.961363                256            0.55   \n",
       "5  0 days 00:16:41.835682               1024            0.55   \n",
       "6  0 days 00:05:34.353218                 32            0.55   \n",
       "7  0 days 00:05:57.166301                 64            0.65   \n",
       "8  0 days 00:15:36.595462                 32            0.10   \n",
       "9  0 days 00:09:50.317570               1024            0.70   \n",
       "10 0 days 00:53:59.059664                128            0.40   \n",
       "11 0 days 00:22:39.688874                128            0.40   \n",
       "12 0 days 01:21:31.811340                 64            0.40   \n",
       "13 0 days 00:26:23.137568                128            0.30   \n",
       "14 0 days 00:38:26.602512               4096            0.25   \n",
       "15 0 days 00:30:20.648595              16384            0.15   \n",
       "16 0 days 00:21:30.076287                256            0.45   \n",
       "17 0 days 00:23:49.291964                256            0.30   \n",
       "18 0 days 00:11:43.112505                256            0.50   \n",
       "19 0 days 00:29:32.970091                256            0.30   \n",
       "20 0 days 00:08:09.767905                256            0.65   \n",
       "21 0 days 00:30:43.017417                256            0.30   \n",
       "22 0 days 00:28:08.128122                256            0.20   \n",
       "23 0 days 00:27:57.687524                256            0.00   \n",
       "24 0 days 00:24:21.572788                 64            0.20   \n",
       "25 0 days 00:10:45.906177                256            0.20   \n",
       "26 0 days 00:20:01.799546                 64            0.35   \n",
       "27 0 days 00:21:54.702404                256            0.10   \n",
       "28 0 days 00:41:23.561240                256            0.10   \n",
       "29 0 days 00:17:11.627752                256            0.00   \n",
       "30 0 days 00:09:43.059911                256            0.15   \n",
       "31 0 days 00:23:58.678394                256            0.25   \n",
       "32 0 days 00:23:29.631759                256            0.15   \n",
       "33 0 days 00:28:04.886808                256            0.10   \n",
       "34 0 days 00:25:25.450477                256            0.05   \n",
       "35 0 days 00:30:44.105766                256            0.05   \n",
       "36 0 days 00:29:28.389604                256            0.05   \n",
       "37 0 days 00:34:16.270886                256            0.10   \n",
       "38 0 days 00:22:49.452187                256            0.15   \n",
       "39 0 days 00:20:11.288224               4096            0.15   \n",
       "40 0 days 00:24:45.212189                256            0.05   \n",
       "41 0 days 00:29:35.233006                256            0.10   \n",
       "42 0 days 00:31:45.051094                256            0.00   \n",
       "43 0 days 00:34:03.832354                256            0.15   \n",
       "44 0 days 00:25:37.504951                256            0.10   \n",
       "45 0 days 01:11:10.008885                256            0.05   \n",
       "46 0 days 00:22:27.119383                256            0.25   \n",
       "47 0 days 00:26:50.337673                256            0.25   \n",
       "48 0 days 00:15:08.833456                256            0.20   \n",
       "49 0 days 00:23:18.362869                256            0.15   \n",
       "\n",
       "    params_embedding_dimension  params_learning_rate  params_window_size  \\\n",
       "0                          128              0.016125                   7   \n",
       "1                          128              0.000299                  12   \n",
       "2                          512              0.043377                  17   \n",
       "3                          128              0.044513                   9   \n",
       "4                          128              0.002235                  10   \n",
       "5                           64              0.021088                  11   \n",
       "6                           32              0.001702                  13   \n",
       "7                           32              0.006701                  11   \n",
       "8                          256              0.000034                   5   \n",
       "9                           64              0.004690                   5   \n",
       "10                         512              0.086243                  19   \n",
       "11                         512              0.077809                  20   \n",
       "12                         512              0.071860                  16   \n",
       "13                         256              0.095726                   8   \n",
       "14                         256              0.018442                   8   \n",
       "15                         256              0.008182                   7   \n",
       "16                         128              0.028616                   9   \n",
       "17                         256              0.087487                   4   \n",
       "18                         128              0.000460                   4   \n",
       "19                         256              0.026249                  14   \n",
       "20                          64              0.009010                   6   \n",
       "21                         256              0.031529                  14   \n",
       "22                         256              0.039100                  14   \n",
       "23                         256              0.033631                  16   \n",
       "24                         256              0.049773                   4   \n",
       "25                          32              0.012186                   4   \n",
       "26                         128              0.058673                   6   \n",
       "27                         256              0.094095                   6   \n",
       "28                         256              0.013318                   6   \n",
       "29                         128              0.016547                   8   \n",
       "30                         128              0.004194                   6   \n",
       "31                         256              0.052707                   4   \n",
       "32                         256              0.065237                   5   \n",
       "33                         256              0.047873                   7   \n",
       "34                         256              0.099189                   7   \n",
       "35                         256              0.095144                   7   \n",
       "36                         256              0.043180                   9   \n",
       "37                         256              0.021193                  10   \n",
       "38                         256              0.048602                   5   \n",
       "39                          32              0.025389                   5   \n",
       "40                          64              0.056631                   5   \n",
       "41                         256              0.047600                   7   \n",
       "42                         256              0.096740                   7   \n",
       "43                         256              0.031242                   5   \n",
       "44                         256              0.059539                   9   \n",
       "45                         512              0.016616                  11   \n",
       "46                         256              0.062581                   6   \n",
       "47                          32              0.066146                   6   \n",
       "48                          64              0.022173                   8   \n",
       "49                         256              0.096121                   5   \n",
       "\n",
       "       state  \n",
       "0   COMPLETE  \n",
       "1   COMPLETE  \n",
       "2   COMPLETE  \n",
       "3   COMPLETE  \n",
       "4   COMPLETE  \n",
       "5   COMPLETE  \n",
       "6   COMPLETE  \n",
       "7   COMPLETE  \n",
       "8   COMPLETE  \n",
       "9   COMPLETE  \n",
       "10  COMPLETE  \n",
       "11  COMPLETE  \n",
       "12  COMPLETE  \n",
       "13  COMPLETE  \n",
       "14  COMPLETE  \n",
       "15  COMPLETE  \n",
       "16  COMPLETE  \n",
       "17  COMPLETE  \n",
       "18  COMPLETE  \n",
       "19  COMPLETE  \n",
       "20  COMPLETE  \n",
       "21  COMPLETE  \n",
       "22  COMPLETE  \n",
       "23  COMPLETE  \n",
       "24  COMPLETE  \n",
       "25  COMPLETE  \n",
       "26  COMPLETE  \n",
       "27  COMPLETE  \n",
       "28  COMPLETE  \n",
       "29  COMPLETE  \n",
       "30  COMPLETE  \n",
       "31  COMPLETE  \n",
       "32  COMPLETE  \n",
       "33  COMPLETE  \n",
       "34  COMPLETE  \n",
       "35  COMPLETE  \n",
       "36  COMPLETE  \n",
       "37  COMPLETE  \n",
       "38  COMPLETE  \n",
       "39  COMPLETE  \n",
       "40  COMPLETE  \n",
       "41  COMPLETE  \n",
       "42  COMPLETE  \n",
       "43  COMPLETE  \n",
       "44  COMPLETE  \n",
       "45  COMPLETE  \n",
       "46  COMPLETE  \n",
       "47  COMPLETE  \n",
       "48  COMPLETE  \n",
       "49  COMPLETE  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
